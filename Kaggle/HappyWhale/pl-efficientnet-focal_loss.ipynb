{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baa0956-160e-47a8-88f8-d3ba10a01a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36957fe-b5d0-489c-88f1-bbaecbccf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.optim import create_optimizer_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f128b24-a6f5-4718-b62c-f8a6a96bc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path('../datasets/kaggle/happy-whale-and-dolphin/')\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "\n",
    "DATA_ROOT_DIR = INPUT_DIR / \"happy-whale-and-dolphin-backfin\"\n",
    "TRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\n",
    "TEST_DIR = DATA_ROOT_DIR / \"test_images\"\n",
    "TRAIN_CSV_PATH = DATA_ROOT_DIR / \"train.csv\"\n",
    "SAMPLE_SUBMISSION_CSV_PATH = DATA_ROOT_DIR / \"sample_submission.csv\"\n",
    "PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"720\" / \"submission.csv\"\n",
    "IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"backfin\" / \"ids_without_backfin.npy\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "ENCODER_CLASSES_PATH = OUTPUT_DIR / \"encoder_classes.npy\"\n",
    "TEST_CSV_PATH = OUTPUT_DIR / \"test.csv\"\n",
    "TRAIN_CSV_ENCODED_FOLDED_PATH = OUTPUT_DIR / \"train_encoded_folded.csv\"\n",
    "CHECKPOINTS_DIR = OUTPUT_DIR / \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac6998e-f4be-45bb-a62e-a4b24941f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "MODEL_NAME = \"tf_efficientnet_b5_ns\"\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 12\n",
    "DEBUG = False\n",
    "SUBMISSION_CSV_PATH = OUTPUT_DIR / (\"pl_\" + MODEL_NAME + f\"_{IMAGE_SIZE}_{FOLD}_focal\" + \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f590ca-4bf7-4dc3-8a5f-386a723c401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(id: str, dir: Path) -> str:\n",
    "    return f\"{dir / id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4685db6f-81fd-4e17-9910-9f587b6f4fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>10938</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1453</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>5158</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4031</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>7726</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species  individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale          10938   \n",
       "1  000562241d384d.jpg      humpback_whale           1453   \n",
       "2  0007c33415ce37.jpg  false_killer_whale           5158   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin           4031   \n",
       "4  00087baf5cef7a.jpg      humpback_whale           7726   \n",
       "\n",
       "                                          image_path  kfold  \n",
       "0  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "1  ../datasets/kaggle/happy-whale-and-dolphin/hap...    1.0  \n",
       "2  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "3  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "4  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG: train_df = pd.read_csv(TRAIN_CSV_PATH)[:10000]\n",
    "else: train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train_df[\"image_path\"] = train_df[\"image\"].apply(get_image_path, dir=TRAIN_DIR)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_df[\"individual_id\"] = encoder.fit_transform(train_df[\"individual_id\"])\n",
    "np.save(ENCODER_CLASSES_PATH, encoder.classes_)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "for fold, (_, val_) in enumerate(skf.split(X=train_df, y=train_df.individual_id)):\n",
    "    train_df.loc[val_, \"kfold\"] = fold\n",
    "    \n",
    "train_df.to_csv(TRAIN_CSV_ENCODED_FOLDED_PATH, index=False)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d70883-c85e-4f57-b958-5fe452d9ed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>image_path</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                         image_path  \\\n",
       "0  000110707af0ba.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "1  0006287ec424cb.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "2  000809ecb2ccad.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "3  00098d1376dab2.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "4  000b8d89c738bd.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "\n",
       "   individual_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sample submission csv as template\n",
    "if DEBUG: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)[:1000]\n",
    "else: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)\n",
    "test_df[\"image_path\"] = test_df[\"image\"].apply(get_image_path, dir=TEST_DIR)\n",
    "\n",
    "test_df.drop(columns=[\"predictions\"], inplace=True)\n",
    "\n",
    "# Dummy id\n",
    "test_df[\"individual_id\"] = 0\n",
    "\n",
    "test_df.to_csv(TEST_CSV_PATH, index=False)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344a1efd-03bf-4e8d-b565-ad6c83aca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Optional[Callable] = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = self.df[\"image\"].values\n",
    "        self.image_paths = self.df[\"image_path\"].values\n",
    "        self.targets = self.df[\"individual_id\"].values\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_name = self.image_names[index]\n",
    "\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = self.targets[index]\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return {\"image_name\": image_name, \"image\": image, \"target\": target}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff4c404-6ff0-4afd-b768-907ac5ea8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_csv_encoded_folded: str,\n",
    "        test_csv: str,\n",
    "        val_fold: float,\n",
    "        image_size: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_df = pd.read_csv(train_csv_encoded_folded)\n",
    "        self.test_df = pd.read_csv(test_csv)\n",
    "        \n",
    "        self.transform = create_transform(\n",
    "            input_size=(self.hparams.image_size, self.hparams.image_size),\n",
    "            crop_pct=1.0,\n",
    "        )\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Split train df using fold\n",
    "            train_df = self.train_df[self.train_df.kfold != self.hparams.val_fold].reset_index(drop=True)\n",
    "            val_df = self.train_df[self.train_df.kfold == self.hparams.val_fold].reset_index(drop=True)\n",
    "\n",
    "            self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform)\n",
    "            self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.train_dataset, train=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.test_dataset)\n",
    "\n",
    "    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=train,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=train,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39657e41-e11a-4698-9863-00017130c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"\n",
    "        This criterion is a implemenation of Focal Loss, which is proposed in \n",
    "        Focal Loss for Dense Object Detection.\n",
    "\n",
    "            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n",
    "\n",
    "        The losses are averaged across observations for each minibatch.\n",
    "\n",
    "        Args:\n",
    "            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n",
    "            gamma(float, double) : gamma > 0; reduces the relative loss for well-classiﬁed examples (p > .5), \n",
    "                                   putting more focus on hard, misclassiﬁed examples\n",
    "            size_average(bool): By default, the losses are averaged over observations for each minibatch.\n",
    "            However, if the field size_average is set to False, the losses are instead summed for each minibatch.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_num=15587, alpha=None, gamma=2, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = Variable(torch.ones(class_num, 1))\n",
    "        else:\n",
    "            if isinstance(alpha, Variable):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = Variable(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.class_num = class_num\n",
    "        self.size_average = size_average\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        N = inputs.size(0)\n",
    "        C = inputs.size(1)\n",
    "        P = F.softmax(inputs,dim=1)\n",
    "\n",
    "        class_mask = inputs.data.new(N, C).fill_(0)\n",
    "        class_mask = Variable(class_mask)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids.data, 1.)\n",
    "        #print(class_mask)\n",
    "\n",
    "\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        alpha = self.alpha[ids.data.view(-1)]\n",
    "\n",
    "        probs = (P*class_mask).sum(1).view(-1,1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "        #print('probs size= {}'.format(probs.size()))\n",
    "        #print(probs)\n",
    "\n",
    "        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p \n",
    "        #print('-----bacth_loss------')\n",
    "        #print(batch_loss)\n",
    "\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b97bbe7b-2c23-4395-a0e1-b7bb275f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "# Added type annotations, device, and 16bit support\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta + m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float,\n",
    "        m: float,\n",
    "        easy_margin: bool,\n",
    "        ls_eps: float,\n",
    "    ):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        # Enable 16 bit precision\n",
    "        cosine = cosine.to(torch.float32)\n",
    "\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa9ef77f-f7f5-45f9-89b1-286d6bffb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        pretrained: bool,\n",
    "        drop_rate: float,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        arc_s: float,\n",
    "        arc_m: float,\n",
    "        arc_easy_margin: bool,\n",
    "        arc_ls_eps: float,\n",
    "        optimizer: str,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        len_train_dl: int,\n",
    "        epochs:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n",
    "        self.embedding = nn.Linear(self.model.get_classifier().in_features, embedding_size)\n",
    "        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        self.arc = ArcMarginProduct(\n",
    "            in_features=embedding_size,\n",
    "            out_features=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            easy_margin=arc_easy_margin,\n",
    "            ls_eps=arc_ls_eps,\n",
    "        )\n",
    "\n",
    "        # self.loss_fn = F.cross_entropy\n",
    "        self.loss_fn = FocalLoss()\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.model(images)\n",
    "        embeddings = self.embedding(features)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=self.hparams.optimizer,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            steps_per_epoch=self.hparams.len_train_dl,\n",
    "            epochs=self.hparams.epochs,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n",
    "        images, targets = batch[\"image\"], batch[\"target\"]\n",
    "\n",
    "        embeddings = self(images)\n",
    "        outputs = self.arc(embeddings, targets, self.device)\n",
    "\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        \n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d98233-7e79-4a51-acc0-b84760a81393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    model_name: str = \"tf_efficientnet_b0\",\n",
    "    pretrained: bool = True,\n",
    "    drop_rate: float = 0.0,\n",
    "    embedding_size: int = 512,\n",
    "    num_classes: int = 15587,\n",
    "    arc_s: float = 30.0,\n",
    "    arc_m: float = 0.5,\n",
    "    arc_easy_margin: bool = False,\n",
    "    arc_ls_eps: float = 0.0,\n",
    "    optimizer: str = \"adam\",\n",
    "    learning_rate: float = 3e-4,\n",
    "    weight_decay: float = 1e-6,\n",
    "    checkpoints_dir: str = str(CHECKPOINTS_DIR),\n",
    "    accumulate_grad_batches: int = 1,\n",
    "    auto_lr_find: bool = False,\n",
    "    auto_scale_batch_size: bool = False,\n",
    "    fast_dev_run: bool = False,\n",
    "    gpus: int = 1,\n",
    "    max_epochs: int = 10,\n",
    "    precision: int = 16,\n",
    "    stochastic_weight_avg: bool = True,\n",
    "):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    datamodule.setup()\n",
    "    len_train_dl = len(datamodule.train_dataloader())\n",
    "\n",
    "    module = LitModule(\n",
    "        model_name=model_name,\n",
    "        pretrained=pretrained,\n",
    "        drop_rate=drop_rate,\n",
    "        embedding_size=embedding_size,\n",
    "        num_classes=num_classes,\n",
    "        arc_s=arc_s,\n",
    "        arc_m=arc_m,\n",
    "        arc_easy_margin=arc_easy_margin,\n",
    "        arc_ls_eps=arc_ls_eps,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        len_train_dl=len_train_dl,\n",
    "        epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        checkpoints_dir,\n",
    "        filename=f\"{model_name}_{image_size}_{val_fold}_focal\",\n",
    "        monitor=\"val_loss\",\n",
    "    )\n",
    "        \n",
    "    trainer = pl.Trainer(\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        auto_lr_find=auto_lr_find,\n",
    "        auto_scale_batch_size=auto_scale_batch_size,\n",
    "        benchmark=True,\n",
    "        callbacks=[model_checkpoint],\n",
    "        deterministic=True,\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        gpus=gpus,\n",
    "        max_epochs=2 if DEBUG else max_epochs,\n",
    "        precision=precision,\n",
    "        stochastic_weight_avg=stochastic_weight_avg,\n",
    "        limit_train_batches=0.1 if DEBUG else 1.0,\n",
    "        limit_val_batches=0.1 if DEBUG else 1.0,\n",
    "    )\n",
    "\n",
    "    trainer.tune(module, datamodule=datamodule)\n",
    "\n",
    "    trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97dd98d7-74ec-44c8-a91c-3f5d263fcacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:59: LightningDeprecationWarning: Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging` directly to the Trainer's `callbacks` argument instead.\n",
      "  \"Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | EfficientNet     | 28.3 M\n",
      "1 | embedding | Linear           | 1.0 M \n",
      "2 | arc       | ArcMarginProduct | 8.0 M \n",
      "3 | loss_fn   | FocalLoss        | 0     \n",
      "-----------------------------------------------\n",
      "37.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "37.4 M    Total params\n",
      "74.741    Total estimated model params size (MB)\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /home/HappyWhale/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 18. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "Global seed set to 42\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bc8f3b0fb049ca8faffa7b7fbdeb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/callbacks/stochastic_weight_avg.py:192: UserWarning: SWA is currently only supported every epoch. Found {'scheduler': <torch.optim.lr_scheduler.OneCycleLR object at 0x7f3c19d739d0>, 'name': None, 'interval': 'step', 'frequency': 1, 'reduce_on_plateau': False, 'monitor': None, 'strict': True, 'opt_idx': None}\n",
      "  rank_zero_warn(f\"SWA is currently only supported every epoch. Found {scheduler_cfg}\")\n",
      "Swapping scheduler `OneCycleLR` for `SWALR`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\n",
    "      val_fold=FOLD,\n",
    "      model_name=MODEL_NAME,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=BATCH_SIZE,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fdd86ee-634c-44f7-be5d-d42688544b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_module(checkpoint_path: str, device: torch.device) -> LitModule:\n",
    "    module = LitModule.load_from_checkpoint(checkpoint_path)\n",
    "    module.to(device)\n",
    "    module.eval()\n",
    "\n",
    "    return module\n",
    "\n",
    "def load_dataloaders(\n",
    "    train_csv_encoded_folded: str,\n",
    "    test_csv: str,\n",
    "    val_fold: float,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    datamodule.setup()\n",
    "\n",
    "    train_dl = datamodule.train_dataloader()\n",
    "    val_dl = datamodule.val_dataloader()\n",
    "    test_dl = datamodule.test_dataloader()\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "def load_encoder() -> LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(ENCODER_CLASSES_PATH, allow_pickle=True)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embeddings(\n",
    "    module: pl.LightningModule, dataloader: DataLoader, encoder: LabelEncoder, stage: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    all_image_names = []\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Creating {stage} embeddings\"):\n",
    "        image_names = batch[\"image_name\"]\n",
    "        images = batch[\"image\"].to(module.device)\n",
    "        targets = batch[\"target\"].to(module.device)\n",
    "\n",
    "        embeddings = module(images)\n",
    "\n",
    "        all_image_names.append(image_names)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     break\n",
    "\n",
    "    all_image_names = np.concatenate(all_image_names)\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    all_embeddings = normalize(all_embeddings, axis=1, norm=\"l2\")\n",
    "    all_targets = encoder.inverse_transform(all_targets)\n",
    "\n",
    "    return all_image_names, all_embeddings, all_targets\n",
    "\n",
    "\n",
    "def create_and_search_index(embedding_size: int, train_embeddings: np.ndarray, val_embeddings: np.ndarray, k: int):\n",
    "    index = faiss.IndexFlatIP(embedding_size)\n",
    "    index.add(train_embeddings)\n",
    "    D, I = index.search(val_embeddings, k=k)  # noqa: E741\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def create_val_targets_df(\n",
    "    train_targets: np.ndarray, val_image_names: np.ndarray, val_targets: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    allowed_targets = np.unique(train_targets)\n",
    "    val_targets_df = pd.DataFrame(np.stack([val_image_names, val_targets], axis=1), columns=[\"image\", \"target\"])\n",
    "    val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), \"target\"] = \"new_individual\"\n",
    "\n",
    "    return val_targets_df\n",
    "\n",
    "\n",
    "def create_distances_df(\n",
    "    image_names: np.ndarray, targets: np.ndarray, D: np.ndarray, I: np.ndarray, stage: str  # noqa: E741\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    distances_df = []\n",
    "    for i, image_name in tqdm(enumerate(image_names), desc=f\"Creating {stage}_df\"):\n",
    "        target = targets[I[i]]\n",
    "        distances = D[i]\n",
    "        subset_preds = pd.DataFrame(np.stack([target, distances], axis=1), columns=[\"target\", \"distances\"])\n",
    "        subset_preds[\"image\"] = image_name\n",
    "        distances_df.append(subset_preds)\n",
    "\n",
    "    distances_df = pd.concat(distances_df).reset_index(drop=True)\n",
    "    distances_df = distances_df.groupby([\"image\", \"target\"]).distances.max().reset_index()\n",
    "    distances_df = distances_df.sort_values(\"distances\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return distances_df\n",
    "\n",
    "\n",
    "def get_best_threshold(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    best_th = 0\n",
    "    best_cv = 0\n",
    "    for th in [0.1 * x for x in range(11)]:\n",
    "        all_preds = get_predictions(valid_df, threshold=th)\n",
    "\n",
    "        cv = 0\n",
    "        for i, row in val_targets_df.iterrows():\n",
    "            target = row.target\n",
    "            preds = all_preds[row.image]\n",
    "            val_targets_df.loc[i, th] = map_per_image(target, preds)\n",
    "\n",
    "        cv = val_targets_df[th].mean()\n",
    "\n",
    "        print(f\"th={th} cv={cv}\")\n",
    "\n",
    "        if cv > best_cv:\n",
    "            best_th = th\n",
    "            best_cv = cv\n",
    "\n",
    "    print(f\"best_th={best_th}\")\n",
    "    print(f\"best_cv={best_cv}\")\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "    print(f\"best_th_adjusted={best_th}\")\n",
    "\n",
    "    return best_th, best_cv\n",
    "\n",
    "\n",
    "def get_predictions(df: pd.DataFrame, threshold: float = 0.2):\n",
    "    sample_list = [\"938b7e931166\", \"5bf17305f073\", \"7593d2aee842\", \"7362d7a01d00\", \"956562ff2888\"]\n",
    "\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating predictions for threshold={threshold}\"):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, \"new_individual\"]\n",
    "        else:\n",
    "            predictions[row.image] = [\"new_individual\", row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# TODO: add types\n",
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def create_predictions_df(test_df: pd.DataFrame, best_th: float) -> pd.DataFrame:\n",
    "    predictions = get_predictions(test_df, best_th)\n",
    "\n",
    "    predictions = pd.Series(predictions).reset_index()\n",
    "    predictions.columns = [\"image\", \"predictions\"]\n",
    "    predictions[\"predictions\"] = predictions[\"predictions\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed613a48-3402-4b6e-98bd-dbe6cf300eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    checkpoint_path: str,\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    k: int = 50,\n",
    "):\n",
    "    module = load_eval_module(checkpoint_path, torch.device(\"cuda\"))\n",
    "\n",
    "    train_dl, val_dl, test_dl = load_dataloaders(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    encoder = load_encoder()\n",
    "\n",
    "    train_image_names, train_embeddings, train_targets = get_embeddings(module, train_dl, encoder, stage=\"train\")\n",
    "    val_image_names, val_embeddings, val_targets = get_embeddings(module, val_dl, encoder, stage=\"val\")\n",
    "    test_image_names, test_embeddings, test_targets = get_embeddings(module, test_dl, encoder, stage=\"test\")\n",
    "\n",
    "    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, val_embeddings, k)  # noqa: E741\n",
    "    print(\"Created index with train_embeddings\")\n",
    "\n",
    "    val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "    print(f\"val_targets_df=\\n{val_targets_df.head()}\")\n",
    "\n",
    "    val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "    print(f\"val_df=\\n{val_df.head()}\")\n",
    "\n",
    "    best_th, best_cv = get_best_threshold(val_targets_df, val_df)\n",
    "    print(f\"val_targets_df=\\n{val_targets_df.describe()}\")\n",
    "\n",
    "    train_embeddings = np.concatenate([train_embeddings, val_embeddings])\n",
    "    train_targets = np.concatenate([train_targets, val_targets])\n",
    "    print(\"Updated train_embeddings and train_targets with val data\")\n",
    "\n",
    "    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, test_embeddings, k)  # noqa: E741\n",
    "    print(\"Created index with train_embeddings\")\n",
    "\n",
    "    test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n",
    "    print(f\"test_df=\\n{test_df.head()}\")\n",
    "\n",
    "    predictions = create_predictions_df(test_df, best_th)\n",
    "    print(f\"predictions.head()={predictions.head()}\")\n",
    "    \n",
    "    # Fix missing predictions\n",
    "    # From https://www.kaggle.com/code/jpbremer/backfins-arcface-tpu-effnet/notebook\n",
    "    public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n",
    "    ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True)\n",
    "\n",
    "    ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n",
    "\n",
    "    predictions = pd.concat(\n",
    "        [\n",
    "            predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n",
    "            public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n",
    "            public_predictions[public_predictions[\"image\"].isin(ids2)],\n",
    "        ]\n",
    "    )\n",
    "    predictions = predictions.drop_duplicates()\n",
    "\n",
    "    predictions.to_csv(SUBMISSION_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5d7e63-3266-45d7-936c-5feb08951ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1dd38b4c1f46d5a7050ae0fdef9bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating train embeddings:   0%|          | 0/2771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6443341d3e84311ac985e24156cdc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val embeddings:   0%|          | 0/693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecc8e5e3d8d4aad82e98e65e26e06f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test embeddings:   0%|          | 0/2329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n",
      "val_targets_df=\n",
      "                image          target\n",
      "0  00021adfb725ed.jpg  new_individual\n",
      "1  0007c33415ce37.jpg    60008f293a2b\n",
      "2  0007d9bca26a99.jpg    4b00fe572063\n",
      "3  00087baf5cef7a.jpg    8e5253662392\n",
      "4  000a8f2d5c316a.jpg    b9907151f66e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdccc97a694246a88afbdc603080384d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_df=\n",
      "                image        target  distances\n",
      "0  1261fd2abe7481.jpg  6a6fa3ec3810   0.998068\n",
      "1  27eed76d08871e.jpg  62ec6fea7ad5   0.997784\n",
      "2  0b84e4c05bb67a.jpg  ce6e37904aa4   0.997598\n",
      "3  0b3e59d395869e.jpg  dd8c756c9cb7   0.996663\n",
      "4  2f16cde13d0d51.jpg  dd8c756c9cb7   0.996660\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd47c5bb3f94d9594e38cf568b41bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.0:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b3a908931647d4afcac04844427286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.0 cv=0.767995590298657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7843a38c3c04c909faa58ccd37e3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.1:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a19ecdca0247439cb44c730975a703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.1 cv=0.767995590298657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2385d1b54c45fab27031021637f376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.2:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fa5d675fb49edb4611fbd9094be50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.2 cv=0.767995590298657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e0d97365254ebb81af042999b08f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.30000000000000004:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5afc22224e4a0aa1e233aa2546d491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.30000000000000004 cv=0.767995590298657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900fa82cac6a4fcf8b08813b7eecfae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.4:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86104f58d3cd47e78f4602215e376448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.4 cv=0.8027520545199439\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e07621b5c5f400e873e03a9ecc1f3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff5ccf170e74210a667f67a3ccb7aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.5 cv=0.8396732812186811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1ff01621cc4cbf872458b90ac1305f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.6000000000000001:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85940b34fb84357a8f0e9492f5cc492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.6000000000000001 cv=0.8164622168771297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdb4c5fbb744336894a1eaeba685b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.7000000000000001:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb54499ce85741c0a68e32f62bcc4bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.7000000000000001 cv=0.7823070755662457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbc5c46141b4ef6843d1f8c7509c3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.8:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbc126e3d584ad282f20d9c852ec552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.8 cv=0.7404550010022048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbfe7c987c4471f82c9ffef5da6b29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.9:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37975b7701d145dcb7c3031428638a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.9 cv=0.6853738224093004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61114cbe310418c8c0bbf5fab036fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=1.0:   0%|          | 0/164916 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a3b72c78b34d27af0eee4f56a1e2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=1.0 cv=0.5426798957706955\n",
      "best_th=0.5\n",
      "best_cv=0.8396732812186811\n",
      "best_th_adjusted=0.5\n",
      "val_targets_df=\n",
      "               0.0          0.1          0.2  0.30000000000000004  \\\n",
      "count  8315.000000  8315.000000  8315.000000          8315.000000   \n",
      "mean      0.767996     0.767996     0.767996             0.767996   \n",
      "std       0.345142     0.345142     0.345142             0.345142   \n",
      "min       0.000000     0.000000     0.000000             0.000000   \n",
      "25%       0.500000     0.500000     0.500000             0.500000   \n",
      "50%       1.000000     1.000000     1.000000             1.000000   \n",
      "75%       1.000000     1.000000     1.000000             1.000000   \n",
      "max       1.000000     1.000000     1.000000             1.000000   \n",
      "\n",
      "               0.4          0.5  0.6000000000000001  0.7000000000000001  \\\n",
      "count  8315.000000  8315.000000         8315.000000         8315.000000   \n",
      "mean      0.802752     0.839673            0.816462            0.782307   \n",
      "std       0.341560     0.333773            0.339159            0.344097   \n",
      "min       0.000000     0.000000            0.000000            0.000000   \n",
      "25%       0.500000     1.000000            1.000000            0.500000   \n",
      "50%       1.000000     1.000000            1.000000            1.000000   \n",
      "75%       1.000000     1.000000            1.000000            1.000000   \n",
      "max       1.000000     1.000000            1.000000            1.000000   \n",
      "\n",
      "               0.8          0.9          1.0  \n",
      "count  8315.000000  8315.000000  8315.000000  \n",
      "mean      0.740455     0.685374     0.542680  \n",
      "std       0.345479     0.339514     0.276514  \n",
      "min       0.000000     0.000000     0.000000  \n",
      "25%       0.500000     0.500000     0.500000  \n",
      "50%       1.000000     0.500000     0.500000  \n",
      "75%       1.000000     1.000000     0.500000  \n",
      "max       1.000000     1.000000     1.000000  \n",
      "Updated train_embeddings and train_targets with val data\n",
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe0ae5328ed4275b11020671db4026c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df=\n",
      "                image        target  distances\n",
      "0  7a785b700b0339.jpg  c93996835aa8   0.996515\n",
      "1  e9abb76a5bed89.jpg  35f898e6595e   0.994784\n",
      "2  f43723eb427018.jpg  600ab1de92d9   0.994415\n",
      "3  a3a9c424ef9f06.jpg  0ed88187dcb5   0.994243\n",
      "4  9fd3d3806e9b29.jpg  c27db73f0e3b   0.993459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6c192b4c0e4cada42ecc83abbb5073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/656316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2a6e39e853487c803e9217ab78ad94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  7a785b700b0339.jpg  c93996835aa8 new_individual ec0b8025d9e6 3ee0a...\n",
      "1  e9abb76a5bed89.jpg  35f898e6595e new_individual 938b7e931166 5bf17...\n",
      "2  f43723eb427018.jpg  600ab1de92d9 new_individual 938b7e931166 5bf17...\n",
      "3  a3a9c424ef9f06.jpg  0ed88187dcb5 new_individual 74a71f03dd87 87f48...\n",
      "4  9fd3d3806e9b29.jpg  c27db73f0e3b new_individual 938b7e931166 5bf17...\n"
     ]
    }
   ],
   "source": [
    "infer(\n",
    "    checkpoint_path=CHECKPOINTS_DIR / f\"{MODEL_NAME}_{IMAGE_SIZE}_{FOLD}_focal.ckpt\", \n",
    "    image_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    val_fold=FOLD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31e827-c07c-40ed-ac8b-05d3f4ed9f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
