{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4baa0956-160e-47a8-88f8-d3ba10a01a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f36957fe-b5d0-489c-88f1-bbaecbccf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.optim import create_optimizer_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f128b24-a6f5-4718-b62c-f8a6a96bc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path('../datasets/kaggle/happy-whale-and-dolphin/')\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "\n",
    "DATA_ROOT_DIR = INPUT_DIR / \"happy-whale-and-dolphin-backfin\"\n",
    "TRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\n",
    "TEST_DIR = DATA_ROOT_DIR / \"test_images\"\n",
    "TRAIN_CSV_PATH = DATA_ROOT_DIR / \"train.csv\"\n",
    "SAMPLE_SUBMISSION_CSV_PATH = DATA_ROOT_DIR / \"sample_submission.csv\"\n",
    "PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"720\" / \"submission.csv\"\n",
    "IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"backfin\" / \"ids_without_backfin.npy\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "ENCODER_CLASSES_PATH = OUTPUT_DIR / \"encoder_classes.npy\"\n",
    "TEST_CSV_PATH = OUTPUT_DIR / \"test.csv\"\n",
    "TRAIN_CSV_ENCODED_FOLDED_PATH = OUTPUT_DIR / \"train_encoded_folded.csv\"\n",
    "CHECKPOINTS_DIR = OUTPUT_DIR / \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac6998e-f4be-45bb-a62e-a4b24941f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 0\n",
    "MODEL_NAME = \"tf_efficientnet_b7_ns\"\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 12\n",
    "DEBUG = False\n",
    "SUBMISSION_CSV_PATH = OUTPUT_DIR / (\"pl_\" + MODEL_NAME + f\"_{IMAGE_SIZE}_{FOLD}_cyclical_\" + \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f590ca-4bf7-4dc3-8a5f-386a723c401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_path(id: str, dir: Path) -> str:\n",
    "    return f\"{dir / id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4685db6f-81fd-4e17-9910-9f587b6f4fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>10938</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1453</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>5158</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4031</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>7726</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species  individual_id  \\\n",
       "0  00021adfb725ed.jpg  melon_headed_whale          10938   \n",
       "1  000562241d384d.jpg      humpback_whale           1453   \n",
       "2  0007c33415ce37.jpg  false_killer_whale           5158   \n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin           4031   \n",
       "4  00087baf5cef7a.jpg      humpback_whale           7726   \n",
       "\n",
       "                                          image_path  kfold  \n",
       "0  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "1  ../datasets/kaggle/happy-whale-and-dolphin/hap...    1.0  \n",
       "2  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "3  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  \n",
       "4  ../datasets/kaggle/happy-whale-and-dolphin/hap...    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG: train_df = pd.read_csv(TRAIN_CSV_PATH)[:10000]\n",
    "else: train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "train_df[\"image_path\"] = train_df[\"image\"].apply(get_image_path, dir=TRAIN_DIR)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "train_df[\"individual_id\"] = encoder.fit_transform(train_df[\"individual_id\"])\n",
    "np.save(ENCODER_CLASSES_PATH, encoder.classes_)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "for fold, (_, val_) in enumerate(skf.split(X=train_df, y=train_df.individual_id)):\n",
    "    train_df.loc[val_, \"kfold\"] = fold\n",
    "    \n",
    "train_df.to_csv(TRAIN_CSV_ENCODED_FOLDED_PATH, index=False)\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d70883-c85e-4f57-b958-5fe452d9ed21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>image_path</th>\n",
       "      <th>individual_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000110707af0ba.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006287ec424cb.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000809ecb2ccad.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00098d1376dab2.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000b8d89c738bd.jpg</td>\n",
       "      <td>../datasets/kaggle/happy-whale-and-dolphin/hap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image                                         image_path  \\\n",
       "0  000110707af0ba.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "1  0006287ec424cb.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "2  000809ecb2ccad.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "3  00098d1376dab2.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "4  000b8d89c738bd.jpg  ../datasets/kaggle/happy-whale-and-dolphin/hap...   \n",
       "\n",
       "   individual_id  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sample submission csv as template\n",
    "if DEBUG: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)[:1000]\n",
    "else: test_df = pd.read_csv(SAMPLE_SUBMISSION_CSV_PATH)\n",
    "test_df[\"image_path\"] = test_df[\"image\"].apply(get_image_path, dir=TEST_DIR)\n",
    "\n",
    "test_df.drop(columns=[\"predictions\"], inplace=True)\n",
    "\n",
    "# Dummy id\n",
    "test_df[\"individual_id\"] = 0\n",
    "\n",
    "test_df.to_csv(TEST_CSV_PATH, index=False)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344a1efd-03bf-4e8d-b565-ad6c83aca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HappyWhaleDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform: Optional[Callable] = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = self.df[\"image\"].values\n",
    "        self.image_paths = self.df[\"image_path\"].values\n",
    "        self.targets = self.df[\"individual_id\"].values\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        image_name = self.image_names[index]\n",
    "\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = self.targets[index]\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return {\"image_name\": image_name, \"image\": image, \"target\": target}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff4c404-6ff0-4afd-b768-907ac5ea8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_csv_encoded_folded: str,\n",
    "        test_csv: str,\n",
    "        val_fold: float,\n",
    "        image_size: int,\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_df = pd.read_csv(train_csv_encoded_folded)\n",
    "        self.test_df = pd.read_csv(test_csv)\n",
    "        \n",
    "        self.transform = create_transform(\n",
    "            input_size=(self.hparams.image_size, self.hparams.image_size),\n",
    "            crop_pct=1.0,\n",
    "        )\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            # Split train df using fold\n",
    "            train_df = self.train_df[self.train_df.kfold != self.hparams.val_fold].reset_index(drop=True)\n",
    "            val_df = self.train_df[self.train_df.kfold == self.hparams.val_fold].reset_index(drop=True)\n",
    "\n",
    "            self.train_dataset = HappyWhaleDataset(train_df, transform=self.transform)\n",
    "            self.val_dataset = HappyWhaleDataset(val_df, transform=self.transform)\n",
    "\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = HappyWhaleDataset(self.test_df, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.train_dataset, train=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.val_dataset)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(self.test_dataset)\n",
    "\n",
    "    def _dataloader(self, dataset: HappyWhaleDataset, train: bool = False) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            shuffle=train,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=train,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97bbe7b-2c23-4395-a0e1-b7bb275f9f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cyclical_FocalLoss(nn.Module):\n",
    "    '''\n",
    "    This loss is intended for single-label classification problems\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, gamma_hc=0, eps: float = 0.1, reduction='mean', epochs=200, \n",
    "                 factor=2):\n",
    "        super(Cyclical_FocalLoss, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_hc = gamma_hc\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "        self.epochs = epochs\n",
    "        self.factor = factor # factor=2 for cyclical, 1 for modified\n",
    "#        self.ceps = ceps\n",
    "#        print(\"Asymetric_Cyclical_FocalLoss: gamma_pos=\", gamma_pos,\" gamma_neg=\",gamma_neg, \n",
    "#              \" eps=\",eps, \" epochs=\", epochs, \" factor=\",factor)\n",
    "\n",
    "    def forward(self, inputs, target, epoch):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "#        print(\"input.size(),target.size()) \",inputs.size(),target.size())\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        if len(list(target.size()))>1:\n",
    "            target = torch.argmax(target, 1)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
    "\n",
    "        # Cyclical\n",
    "#        eta = abs(1 - self.factor*epoch/(self.epochs-1))\n",
    "        if self.factor*epoch < self.epochs:\n",
    "            eta = 1 - self.factor *epoch/(self.epochs-1)\n",
    "        elif self.factor == 1.0:\n",
    "            eta = 0\n",
    "        else:\n",
    "            eta = (self.factor*epoch/(self.epochs-1) - 1.0)/(self.factor - 1.0) \n",
    "\n",
    "        # ASL weights\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        positive_w = torch.pow(1 + xs_pos,self.gamma_hc * targets)\n",
    "        log_preds = log_preds * ((1 - eta)* asymmetric_w + eta * positive_w)\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "# Added type annotations, device, and 16bit support\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta + m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float,\n",
    "        m: float,\n",
    "        easy_margin: bool,\n",
    "        ls_eps: float,\n",
    "    ):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        # Enable 16 bit precision\n",
    "        cosine = cosine.to(torch.float32)\n",
    "\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa9ef77f-f7f5-45f9-89b1-286d6bffb173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        pretrained: bool,\n",
    "        drop_rate: float,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        arc_s: float,\n",
    "        arc_m: float,\n",
    "        arc_easy_margin: bool,\n",
    "        arc_ls_eps: float,\n",
    "        optimizer: str,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        len_train_dl: int,\n",
    "        epochs:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n",
    "        self.embedding = nn.Linear(self.model.get_classifier().in_features, embedding_size)\n",
    "        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        self.arc = ArcMarginProduct(\n",
    "            in_features=embedding_size,\n",
    "            out_features=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            easy_margin=arc_easy_margin,\n",
    "            ls_eps=arc_ls_eps,\n",
    "        )\n",
    "\n",
    "        # self.loss_fn = F.cross_entropy\n",
    "        self.loss_fn = Cyclical_FocalLoss()\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.model(images)\n",
    "        embeddings = self.embedding(features)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=self.hparams.optimizer,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            steps_per_epoch=self.hparams.len_train_dl,\n",
    "            epochs=self.hparams.epochs,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n",
    "        images, targets = batch[\"image\"], batch[\"target\"]\n",
    "\n",
    "        embeddings = self(images)\n",
    "        outputs = self.arc(embeddings, targets, self.device)\n",
    "\n",
    "        # loss = self.loss_fn(outputs, targets)\n",
    "        loss = self.loss_fn(outputs, targets, epoch=10)\n",
    "        \n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d98233-7e79-4a51-acc0-b84760a81393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    model_name: str = \"tf_efficientnet_b0\",\n",
    "    pretrained: bool = True,\n",
    "    drop_rate: float = 0.0,\n",
    "    embedding_size: int = 512,\n",
    "    num_classes: int = 15587,\n",
    "    arc_s: float = 30.0,\n",
    "    arc_m: float = 0.5,\n",
    "    arc_easy_margin: bool = False,\n",
    "    arc_ls_eps: float = 0.0,\n",
    "    optimizer: str = \"adam\",\n",
    "    learning_rate: float = 3e-4,\n",
    "    weight_decay: float = 1e-6,\n",
    "    checkpoints_dir: str = str(CHECKPOINTS_DIR),\n",
    "    accumulate_grad_batches: int = 1,\n",
    "    auto_lr_find: bool = False,\n",
    "    auto_scale_batch_size: bool = False,\n",
    "    fast_dev_run: bool = False,\n",
    "    gpus: int = 1,\n",
    "    max_epochs: int = 10,\n",
    "    precision: int = 16,\n",
    "    stochastic_weight_avg: bool = True,\n",
    "):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    \n",
    "    datamodule.setup()\n",
    "    len_train_dl = len(datamodule.train_dataloader())\n",
    "\n",
    "    module = LitModule(\n",
    "        model_name=model_name,\n",
    "        pretrained=pretrained,\n",
    "        drop_rate=drop_rate,\n",
    "        embedding_size=embedding_size,\n",
    "        num_classes=num_classes,\n",
    "        arc_s=arc_s,\n",
    "        arc_m=arc_m,\n",
    "        arc_easy_margin=arc_easy_margin,\n",
    "        arc_ls_eps=arc_ls_eps,\n",
    "        optimizer=optimizer,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        len_train_dl=len_train_dl,\n",
    "        epochs=max_epochs\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        checkpoints_dir,\n",
    "        filename=f\"{model_name}_{image_size}_{val_fold}_cyclical\",\n",
    "        monitor=\"val_loss\",\n",
    "    )\n",
    "        \n",
    "    trainer = pl.Trainer(\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        auto_lr_find=auto_lr_find,\n",
    "        auto_scale_batch_size=auto_scale_batch_size,\n",
    "        benchmark=True,\n",
    "        callbacks=[model_checkpoint],\n",
    "        deterministic=True,\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        gpus=gpus,\n",
    "        max_epochs=2 if DEBUG else max_epochs,\n",
    "        precision=precision,\n",
    "        stochastic_weight_avg=stochastic_weight_avg,\n",
    "        limit_train_batches=0.1 if DEBUG else 1.0,\n",
    "        limit_val_batches=0.1 if DEBUG else 1.0,\n",
    "    )\n",
    "\n",
    "    trainer.tune(module, datamodule=datamodule)\n",
    "\n",
    "    trainer.fit(module, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd98d7-74ec-44c8-a91c-3f5d263fcacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:59: LightningDeprecationWarning: Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.stochastic_weight_avg.StochasticWeightAveraging` directly to the Trainer's `callbacks` argument instead.\n",
      "  \"Setting `Trainer(stochastic_weight_avg=True)` is deprecated in v1.5 and will be removed in v1.7.\"\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | EfficientNet       | 63.8 M\n",
      "1 | embedding | Linear             | 1.3 M \n",
      "2 | arc       | ArcMarginProduct   | 8.0 M \n",
      "3 | loss_fn   | Cyclical_FocalLoss | 0     \n",
      "-------------------------------------------------\n",
      "73.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "73.1 M    Total params\n",
      "146.157   Total estimated model params size (MB)\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:623: UserWarning: Checkpoint directory /home/HappyWhale/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/utilities/data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 18. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "Global seed set to 42\n",
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:112: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a923dc255df47b3b145092667a76f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(\n",
    "      val_fold=FOLD,\n",
    "      model_name=MODEL_NAME,\n",
    "      image_size=IMAGE_SIZE,\n",
    "      batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdd86ee-634c-44f7-be5d-d42688544b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_module(checkpoint_path: str, device: torch.device) -> LitModule:\n",
    "    module = LitModule.load_from_checkpoint(checkpoint_path)\n",
    "    module.to(device)\n",
    "    module.eval()\n",
    "\n",
    "    return module\n",
    "\n",
    "def load_dataloaders(\n",
    "    train_csv_encoded_folded: str,\n",
    "    test_csv: str,\n",
    "    val_fold: float,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    datamodule.setup()\n",
    "\n",
    "    train_dl = datamodule.train_dataloader()\n",
    "    val_dl = datamodule.val_dataloader()\n",
    "    test_dl = datamodule.test_dataloader()\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "def load_encoder() -> LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(ENCODER_CLASSES_PATH, allow_pickle=True)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embeddings(\n",
    "    module: pl.LightningModule, dataloader: DataLoader, encoder: LabelEncoder, stage: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    all_image_names = []\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Creating {stage} embeddings\"):\n",
    "        image_names = batch[\"image_name\"]\n",
    "        images = batch[\"image\"].to(module.device)\n",
    "        targets = batch[\"target\"].to(module.device)\n",
    "\n",
    "        embeddings = module(images)\n",
    "\n",
    "        all_image_names.append(image_names)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     break\n",
    "\n",
    "    all_image_names = np.concatenate(all_image_names)\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    all_embeddings = normalize(all_embeddings, axis=1, norm=\"l2\")\n",
    "    all_targets = encoder.inverse_transform(all_targets)\n",
    "\n",
    "    return all_image_names, all_embeddings, all_targets\n",
    "\n",
    "\n",
    "def create_and_search_index(embedding_size: int, train_embeddings: np.ndarray, val_embeddings: np.ndarray, k: int):\n",
    "    index = faiss.IndexFlatIP(embedding_size)\n",
    "    index.add(train_embeddings)\n",
    "    D, I = index.search(val_embeddings, k=k)  # noqa: E741\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def create_val_targets_df(\n",
    "    train_targets: np.ndarray, val_image_names: np.ndarray, val_targets: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    allowed_targets = np.unique(train_targets)\n",
    "    val_targets_df = pd.DataFrame(np.stack([val_image_names, val_targets], axis=1), columns=[\"image\", \"target\"])\n",
    "    val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), \"target\"] = \"new_individual\"\n",
    "\n",
    "    return val_targets_df\n",
    "\n",
    "\n",
    "def create_distances_df(\n",
    "    image_names: np.ndarray, targets: np.ndarray, D: np.ndarray, I: np.ndarray, stage: str  # noqa: E741\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    distances_df = []\n",
    "    for i, image_name in tqdm(enumerate(image_names), desc=f\"Creating {stage}_df\"):\n",
    "        target = targets[I[i]]\n",
    "        distances = D[i]\n",
    "        subset_preds = pd.DataFrame(np.stack([target, distances], axis=1), columns=[\"target\", \"distances\"])\n",
    "        subset_preds[\"image\"] = image_name\n",
    "        distances_df.append(subset_preds)\n",
    "\n",
    "    distances_df = pd.concat(distances_df).reset_index(drop=True)\n",
    "    distances_df = distances_df.groupby([\"image\", \"target\"]).distances.max().reset_index()\n",
    "    distances_df = distances_df.sort_values(\"distances\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return distances_df\n",
    "\n",
    "\n",
    "def get_best_threshold(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    best_th = 0\n",
    "    best_cv = 0\n",
    "    for th in [0.1 * x for x in range(11)]:\n",
    "        all_preds = get_predictions(valid_df, threshold=th)\n",
    "\n",
    "        cv = 0\n",
    "        for i, row in val_targets_df.iterrows():\n",
    "            target = row.target\n",
    "            preds = all_preds[row.image]\n",
    "            val_targets_df.loc[i, th] = map_per_image(target, preds)\n",
    "\n",
    "        cv = val_targets_df[th].mean()\n",
    "\n",
    "        print(f\"th={th} cv={cv}\")\n",
    "\n",
    "        if cv > best_cv:\n",
    "            best_th = th\n",
    "            best_cv = cv\n",
    "\n",
    "    print(f\"best_th={best_th}\")\n",
    "    print(f\"best_cv={best_cv}\")\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "    print(f\"best_th_adjusted={best_th}\")\n",
    "\n",
    "    return best_th, best_cv\n",
    "\n",
    "\n",
    "def get_predictions(df: pd.DataFrame, threshold: float = 0.2):\n",
    "    sample_list = [\"938b7e931166\", \"5bf17305f073\", \"7593d2aee842\", \"7362d7a01d00\", \"956562ff2888\"]\n",
    "\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating predictions for threshold={threshold}\"):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, \"new_individual\"]\n",
    "        else:\n",
    "            predictions[row.image] = [\"new_individual\", row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# TODO: add types\n",
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def create_predictions_df(test_df: pd.DataFrame, best_th: float) -> pd.DataFrame:\n",
    "    predictions = get_predictions(test_df, best_th)\n",
    "\n",
    "    predictions = pd.Series(predictions).reset_index()\n",
    "    predictions.columns = [\"image\", \"predictions\"]\n",
    "    predictions[\"predictions\"] = predictions[\"predictions\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed613a48-3402-4b6e-98bd-dbe6cf300eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    checkpoint_path: str,\n",
    "    train_csv_encoded_folded: str = str(TRAIN_CSV_ENCODED_FOLDED_PATH),\n",
    "    test_csv: str = str(TEST_CSV_PATH),\n",
    "    val_fold: float = 0.0,\n",
    "    image_size: int = 256,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    k: int = 50,\n",
    "):\n",
    "    module = load_eval_module(checkpoint_path, torch.device(\"cuda\"))\n",
    "\n",
    "    train_dl, val_dl, test_dl = load_dataloaders(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    encoder = load_encoder()\n",
    "\n",
    "    train_image_names, train_embeddings, train_targets = get_embeddings(module, train_dl, encoder, stage=\"train\")\n",
    "    val_image_names, val_embeddings, val_targets = get_embeddings(module, val_dl, encoder, stage=\"val\")\n",
    "    test_image_names, test_embeddings, test_targets = get_embeddings(module, test_dl, encoder, stage=\"test\")\n",
    "\n",
    "    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, val_embeddings, k)  # noqa: E741\n",
    "    print(\"Created index with train_embeddings\")\n",
    "\n",
    "    val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "    print(f\"val_targets_df=\\n{val_targets_df.head()}\")\n",
    "\n",
    "    val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "    print(f\"val_df=\\n{val_df.head()}\")\n",
    "\n",
    "    best_th, best_cv = get_best_threshold(val_targets_df, val_df)\n",
    "    print(f\"val_targets_df=\\n{val_targets_df.describe()}\")\n",
    "\n",
    "    train_embeddings = np.concatenate([train_embeddings, val_embeddings])\n",
    "    train_targets = np.concatenate([train_targets, val_targets])\n",
    "    print(\"Updated train_embeddings and train_targets with val data\")\n",
    "\n",
    "    D, I = create_and_search_index(module.hparams.embedding_size, train_embeddings, test_embeddings, k)  # noqa: E741\n",
    "    print(\"Created index with train_embeddings\")\n",
    "\n",
    "    test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n",
    "    print(f\"test_df=\\n{test_df.head()}\")\n",
    "\n",
    "    predictions = create_predictions_df(test_df, best_th)\n",
    "    print(f\"predictions.head()={predictions.head()}\")\n",
    "    \n",
    "    # Fix missing predictions\n",
    "    # From https://www.kaggle.com/code/jpbremer/backfins-arcface-tpu-effnet/notebook\n",
    "    public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n",
    "    ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True)\n",
    "\n",
    "    ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n",
    "\n",
    "    predictions = pd.concat(\n",
    "        [\n",
    "            predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n",
    "            public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n",
    "            public_predictions[public_predictions[\"image\"].isin(ids2)],\n",
    "        ]\n",
    "    )\n",
    "    predictions = predictions.drop_duplicates()\n",
    "\n",
    "    predictions.to_csv(SUBMISSION_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f5d7e63-3266-45d7-936c-5feb08951ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a4ba3411fa472eac88b7d1fde8f859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating train embeddings:   0%|          | 0/2771 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a8a33629fb4772b5bdc542e2aa06cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val embeddings:   0%|          | 0/693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8026e683f4741a9870a41bb74840127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test embeddings:   0%|          | 0/2329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n",
      "val_targets_df=\n",
      "                image          target\n",
      "0  00021adfb725ed.jpg  new_individual\n",
      "1  0007c33415ce37.jpg    60008f293a2b\n",
      "2  0007d9bca26a99.jpg    4b00fe572063\n",
      "3  00087baf5cef7a.jpg    8e5253662392\n",
      "4  000a8f2d5c316a.jpg    b9907151f66e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328f8142bad94172a96c691d0fa654f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_df=\n",
      "                image        target  distances\n",
      "0  3bd1da8f91e416.jpg  4a67e64bd3b7   0.999850\n",
      "1  1b3495ba16b398.jpg  bbeac4b2964e   0.999826\n",
      "2  138fe1d758eb86.jpg  84d91f8c1f99   0.999784\n",
      "3  18b27f84a0ab8f.jpg  bbeac4b2964e   0.999782\n",
      "4  0723310846e133.jpg  4a67e64bd3b7   0.999774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af74d2f234243c7a135b940bb40a187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.0:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b949b106eed49bebb8903454064fd42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.0 cv=0.7357065544197233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e64035226549419a1df80a9af13c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.1:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40b366adef848e48732039da1c4d7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.1 cv=0.7357065544197233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabd08c47613493c8b16260248678431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.2:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ba64e8dd684ce79a5ddf666f9783e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.2 cv=0.7357065544197233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3689a96069a4aeaa62d32b6e51ff203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.30000000000000004:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33854db6b062408d8caf6921a2d2a0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.30000000000000004 cv=0.7357065544197233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf43a332f594fed9ce4810d9a4047ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.4:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026005e70cbc4c41bc9a9174908eafd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.4 cv=0.7364882742032471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428efb9eb30a42b7b9c05825ac17fcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bd3259d1e140959717ab9acdb8f7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.5 cv=0.7705231509320505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ba5b280cd04b7b82f157bcf3a9433b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.6000000000000001:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82017f857d654625a225daa51d1e01a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.6000000000000001 cv=0.7978833433553818\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb87319739b48278222c274c63b4751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.7000000000000001:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98a12d3b3b44836bda56f66f4d53729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.7000000000000001 cv=0.7677570655441972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369b851db1c5473b88d3efbf1f52d680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.8:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ada6bf0a4a4490b797e7cfa16b2074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.8 cv=0.7227781118460613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428344791e5e444db5edfc9293b6c0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.9:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f60d62f16d43349d7bf1f86d4b8190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.9 cv=0.673770294648226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b175ac7da0e43de876b2ed331cc28a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=1.0:   0%|          | 0/201187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a71662d8754216947f80614d5e0b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=1.0 cv=0.5278893565844859\n",
      "best_th=0.6000000000000001\n",
      "best_cv=0.7978833433553818\n",
      "best_th_adjusted=0.5\n",
      "val_targets_df=\n",
      "               0.0          0.1          0.2  0.30000000000000004  \\\n",
      "count  8315.000000  8315.000000  8315.000000          8315.000000   \n",
      "mean      0.735707     0.735707     0.735707             0.735707   \n",
      "std       0.365353     0.365353     0.365353             0.365353   \n",
      "min       0.000000     0.000000     0.000000             0.000000   \n",
      "25%       0.500000     0.500000     0.500000             0.500000   \n",
      "50%       1.000000     1.000000     1.000000             1.000000   \n",
      "75%       1.000000     1.000000     1.000000             1.000000   \n",
      "max       1.000000     1.000000     1.000000             1.000000   \n",
      "\n",
      "               0.4          0.5  0.6000000000000001  0.7000000000000001  \\\n",
      "count  8315.000000  8315.000000         8315.000000         8315.000000   \n",
      "mean      0.736488     0.770523            0.797883            0.767757   \n",
      "std       0.365383     0.365056            0.362483            0.365201   \n",
      "min       0.000000     0.000000            0.000000            0.000000   \n",
      "25%       0.500000     0.500000            0.500000            0.500000   \n",
      "50%       1.000000     1.000000            1.000000            1.000000   \n",
      "75%       1.000000     1.000000            1.000000            1.000000   \n",
      "max       1.000000     1.000000            1.000000            1.000000   \n",
      "\n",
      "               0.8          0.9          1.0  \n",
      "count  8315.000000  8315.000000  8315.000000  \n",
      "mean      0.722778     0.673770     0.527889  \n",
      "std       0.364618     0.357597     0.290428  \n",
      "min       0.000000     0.000000     0.000000  \n",
      "25%       0.500000     0.500000     0.500000  \n",
      "50%       1.000000     0.500000     0.500000  \n",
      "75%       1.000000     1.000000     0.500000  \n",
      "max       1.000000     1.000000     1.000000  \n",
      "Updated train_embeddings and train_targets with val data\n",
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfd7d2707ef48f0a6a0d3f4c9fd456d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df=\n",
      "                image        target  distances\n",
      "0  82dfa332e68cd3.jpg  4a67e64bd3b7   0.999679\n",
      "1  1538ca07ca49a7.jpg  4a67e64bd3b7   0.999672\n",
      "2  d011dce211e72a.jpg  4a67e64bd3b7   0.999586\n",
      "3  af1bdb2029503d.jpg  4a67e64bd3b7   0.999562\n",
      "4  21e8b22a31a39e.jpg  4a67e64bd3b7   0.999560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0723bc537154e28912e7145592f3e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/799609 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45004031818f4f358d972fe191175046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  82dfa332e68cd3.jpg  4a67e64bd3b7 new_individual 938b7e931166 5bf17...\n",
      "1  1538ca07ca49a7.jpg  4a67e64bd3b7 new_individual 938b7e931166 5bf17...\n",
      "2  d011dce211e72a.jpg  4a67e64bd3b7 new_individual 938b7e931166 5bf17...\n",
      "3  af1bdb2029503d.jpg  4a67e64bd3b7 new_individual 938b7e931166 5bf17...\n",
      "4  21e8b22a31a39e.jpg  4a67e64bd3b7 new_individual 938b7e931166 5bf17...\n"
     ]
    }
   ],
   "source": [
    "infer(\n",
    "    checkpoint_path=CHECKPOINTS_DIR / f\"{MODEL_NAME}_{IMAGE_SIZE}_{FOLD}_cyclical.ckpt\", \n",
    "    image_size=IMAGE_SIZE, \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31e827-c07c-40ed-ac8b-05d3f4ed9f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
