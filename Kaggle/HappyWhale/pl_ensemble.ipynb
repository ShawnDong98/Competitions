{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9b2632-396b-45bd-af38-8d615130fa92",
   "metadata": {},
   "source": [
    "# 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8c29c0-4dae-4d67-903a-a1dc841f487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.optim import create_optimizer_v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e71f16-6049-4daa-9f1d-75b8cf2a4a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py\n",
    "# Added type annotations, device, and 16bit support\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta + m)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        s: float,\n",
    "        m: float,\n",
    "        easy_margin: bool,\n",
    "        ls_eps: float,\n",
    "    ):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input: torch.Tensor, label: torch.Tensor, device: str = \"cuda\") -> torch.Tensor:\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        # Enable 16 bit precision\n",
    "        cosine = cosine.to(torch.float32)\n",
    "\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd39e69-dd63-4423-9f02-74cb34b58c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        pretrained: bool,\n",
    "        drop_rate: float,\n",
    "        embedding_size: int,\n",
    "        num_classes: int,\n",
    "        arc_s: float,\n",
    "        arc_m: float,\n",
    "        arc_easy_margin: bool,\n",
    "        arc_ls_eps: float,\n",
    "        optimizer: str,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        len_train_dl: int,\n",
    "        epochs:int\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n",
    "        self.embedding = nn.Linear(self.model.get_classifier().in_features, embedding_size)\n",
    "        self.model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        self.arc = ArcMarginProduct(\n",
    "            in_features=embedding_size,\n",
    "            out_features=num_classes,\n",
    "            s=arc_s,\n",
    "            m=arc_m,\n",
    "            easy_margin=arc_easy_margin,\n",
    "            ls_eps=arc_ls_eps,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = F.cross_entropy\n",
    "\n",
    "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.model(images)\n",
    "        embeddings = self.embedding(features)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=self.hparams.optimizer,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.hparams.learning_rate,\n",
    "            steps_per_epoch=self.hparams.len_train_dl,\n",
    "            epochs=self.hparams.epochs,\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def _step(self, batch: Dict[str, torch.Tensor], step: str) -> torch.Tensor:\n",
    "        images, targets = batch[\"image\"], batch[\"target\"]\n",
    "\n",
    "        embeddings = self(images)\n",
    "        outputs = self.arc(embeddings, targets, self.device)\n",
    "\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        \n",
    "        self.log(f\"{step}_loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3375792c-b67b-40c9-870d-2b86c471a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_module(checkpoint_path: str, device: torch.device) -> LitModule:\n",
    "    module = LitModule.load_from_checkpoint(checkpoint_path)\n",
    "    module.to(device)\n",
    "    module.eval()\n",
    "\n",
    "    return module\n",
    "\n",
    "def load_dataloaders(\n",
    "    train_csv_encoded_folded: str,\n",
    "    test_csv: str,\n",
    "    val_fold: float,\n",
    "    image_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "\n",
    "    datamodule = LitDataModule(\n",
    "        train_csv_encoded_folded=train_csv_encoded_folded,\n",
    "        test_csv=test_csv,\n",
    "        val_fold=val_fold,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    datamodule.setup()\n",
    "\n",
    "    train_dl = datamodule.train_dataloader()\n",
    "    val_dl = datamodule.val_dataloader()\n",
    "    test_dl = datamodule.test_dataloader()\n",
    "\n",
    "    return train_dl, val_dl, test_dl\n",
    "\n",
    "\n",
    "def load_encoder() -> LabelEncoder:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load(ENCODER_CLASSES_PATH, allow_pickle=True)\n",
    "\n",
    "    return encoder\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_embeddings(\n",
    "    module: pl.LightningModule, dataloader: DataLoader, encoder: LabelEncoder, stage: str\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "    all_image_names = []\n",
    "    all_embeddings = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Creating {stage} embeddings\"):\n",
    "        image_names = batch[\"image_name\"]\n",
    "        images = batch[\"image\"].to(module.device)\n",
    "        targets = batch[\"target\"].to(module.device)\n",
    "\n",
    "        embeddings = module(images)\n",
    "\n",
    "        all_image_names.append(image_names)\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        \n",
    "        # if DEBUG:\n",
    "        #     break\n",
    "\n",
    "    all_image_names = np.concatenate(all_image_names)\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "\n",
    "    all_embeddings = normalize(all_embeddings, axis=1, norm=\"l2\")\n",
    "    all_targets = encoder.inverse_transform(all_targets)\n",
    "\n",
    "    return all_image_names, all_embeddings, all_targets\n",
    "\n",
    "\n",
    "def create_and_search_index(embedding_size: int, train_embeddings: np.ndarray, val_embeddings: np.ndarray, k: int):\n",
    "    index = faiss.IndexFlatIP(embedding_size)\n",
    "    index.add(train_embeddings)\n",
    "    D, I = index.search(val_embeddings, k=k)  # noqa: E741\n",
    "\n",
    "    return D, I\n",
    "\n",
    "\n",
    "def create_val_targets_df(\n",
    "    train_targets: np.ndarray, val_image_names: np.ndarray, val_targets: np.ndarray\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    allowed_targets = np.unique(train_targets)\n",
    "    val_targets_df = pd.DataFrame(np.stack([val_image_names, val_targets], axis=1), columns=[\"image\", \"target\"])\n",
    "    val_targets_df.loc[~val_targets_df.target.isin(allowed_targets), \"target\"] = \"new_individual\"\n",
    "\n",
    "    return val_targets_df\n",
    "\n",
    "\n",
    "def create_distances_df(\n",
    "    image_names: np.ndarray, targets: np.ndarray, D: np.ndarray, I: np.ndarray, stage: str  # noqa: E741\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    distances_df = []\n",
    "    for i, image_name in tqdm(enumerate(image_names), desc=f\"Creating {stage}_df\"):\n",
    "        target = targets[I[i]]\n",
    "        distances = D[i]\n",
    "        subset_preds = pd.DataFrame(np.stack([target, distances], axis=1), columns=[\"target\", \"distances\"])\n",
    "        subset_preds[\"image\"] = image_name\n",
    "        distances_df.append(subset_preds)\n",
    "\n",
    "    distances_df = pd.concat(distances_df).reset_index(drop=True)\n",
    "    distances_df = distances_df.groupby([\"image\", \"target\"]).distances.max().reset_index()\n",
    "    distances_df = distances_df.sort_values(\"distances\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return distances_df\n",
    "\n",
    "\n",
    "def get_cv(val_targets_df: pd.DataFrame, all_preds) -> Tuple[float, float]:\n",
    "    # all_preds = get_predictions(valid_df, threshold=0.5)\n",
    "    cv = 0\n",
    "    for i, row in val_targets_df.iterrows():\n",
    "        target = row.target\n",
    "        preds = all_preds[row.image]\n",
    "        val_targets_df.loc[i, 0.5] = map_per_image(target, preds)\n",
    "\n",
    "    cv = val_targets_df[0.5].mean()\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "\n",
    "    return cv\n",
    "\n",
    "\n",
    "\n",
    "def get_best_threshold(val_targets_df: pd.DataFrame, valid_df: pd.DataFrame) -> Tuple[float, float]:\n",
    "    best_th = 0\n",
    "    best_cv = 0\n",
    "    for th in [0.01 * x for x in range(40, 60, 2)]:\n",
    "        all_preds = get_predictions(valid_df, threshold=th)\n",
    "\n",
    "        cv = 0\n",
    "        for i, row in val_targets_df.iterrows():\n",
    "            target = row.target\n",
    "            preds = all_preds[row.image]\n",
    "            val_targets_df.loc[i, th] = map_per_image(target, preds)\n",
    "\n",
    "        cv = val_targets_df[th].mean()\n",
    "\n",
    "        print(f\"th={th} cv={cv}\")\n",
    "\n",
    "        if cv > best_cv:\n",
    "            best_th = th\n",
    "            best_cv = cv\n",
    "\n",
    "    print(f\"best_th={best_th}\")\n",
    "    print(f\"best_cv={best_cv}\")\n",
    "\n",
    "    # Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\n",
    "    val_targets_df[\"is_new_individual\"] = val_targets_df.target == \"new_individual\"\n",
    "    val_scores = val_targets_df.groupby(\"is_new_individual\").mean().T\n",
    "    val_scores[\"adjusted_cv\"] = val_scores[True] * 0.1 + val_scores[False] * 0.9\n",
    "    best_th = val_scores[\"adjusted_cv\"].idxmax()\n",
    "    print(f\"best_th_adjusted={best_th}\")\n",
    "\n",
    "    return best_th, best_cv\n",
    "\n",
    "\n",
    "def get_predictions(df: pd.DataFrame, threshold: float = 0.2):\n",
    "    sample_list = [\"938b7e931166\", \"5bf17305f073\", \"7593d2aee842\", \"7362d7a01d00\", \"956562ff2888\"]\n",
    "\n",
    "    predictions = {}\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Creating predictions for threshold={threshold}\"):\n",
    "        if row.image in predictions:\n",
    "            if len(predictions[row.image]) == 5:\n",
    "                continue\n",
    "            predictions[row.image].append(row.target)\n",
    "        elif row.distances > threshold:\n",
    "            predictions[row.image] = [row.target, \"new_individual\"]\n",
    "        else:\n",
    "            predictions[row.image] = [\"new_individual\", row.target]\n",
    "\n",
    "    for x in tqdm(predictions):\n",
    "        if len(predictions[x]) < 5:\n",
    "            remaining = [y for y in sample_list if y not in predictions]\n",
    "            predictions[x] = predictions[x] + remaining\n",
    "            predictions[x] = predictions[x][:5]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# TODO: add types\n",
    "def map_per_image(label, predictions):\n",
    "    \"\"\"Computes the precision score of one image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : string\n",
    "            The true label of the image\n",
    "    predictions : list\n",
    "            A list of predicted elements (order does matter, 5 predictions allowed per image)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return 1 / (predictions[:5].index(label) + 1)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def create_predictions_df(test_df: pd.DataFrame, best_th: float) -> pd.DataFrame:\n",
    "    predictions = get_predictions(test_df, best_th)\n",
    "\n",
    "    predictions = pd.Series(predictions).reset_index()\n",
    "    predictions.columns = [\"image\", \"predictions\"]\n",
    "    predictions[\"predictions\"] = predictions[\"predictions\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179eb8f1-9a58-4434-814c-bde9170386b8",
   "metadata": {},
   "source": [
    "# 找到合适的加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61a6c6cb-e7ab-4783-9e26-fec77b25b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "convnext:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "convnext:  {'train': {'train_image_names': array(['00021adfb725ed.jpg', '000562241d384d.jpg', '0007c33415ce37.jpg',\n",
      "       ..., 'ffae18d2939ffc.jpg', 'ffaf6c062ccdef.jpg',\n",
      "       'ffb1ddd9a59530.jpg'], dtype='<U18'), 'train_embeddings': array([[ 0.03500184,  0.02693112, -0.09545888, ...,  0.0084056 ,\n",
      "         0.04649101,  0.02836194],\n",
      "       [ 0.07109264,  0.02076571,  0.04391202, ...,  0.0821337 ,\n",
      "        -0.03508933, -0.00155115],\n",
      "       [ 0.01291985, -0.01428259, -0.04355193, ...,  0.07608046,\n",
      "         0.02878092, -0.0575893 ],\n",
      "       ...,\n",
      "       [ 0.08326291, -0.02447291,  0.03061166, ...,  0.0741178 ,\n",
      "        -0.06301463,  0.02398695],\n",
      "       [ 0.0423771 ,  0.02825621, -0.06514937, ..., -0.06134047,\n",
      "         0.05141734,  0.09020478],\n",
      "       [-0.01103833, -0.03521555, -0.02877165, ...,  0.02577648,\n",
      "         0.02116164, -0.00764073]], dtype=float32), 'train_targets': array(['cadddb1636b9', '1a71fbb72250', '60008f293a2b', ...,\n",
      "       '1a71fbb72250', '938167f9ea20', 'f92fcbf8f42d'], dtype=object)}, 'val': {'val_image_names': array(['00b0f5aa111833.jpg', '00c021e7dc6da2.jpg', '00de478885ca7c.jpg',\n",
      "       ..., 'fff8b32daff17e.jpg', 'fff94675cc1aef.jpg',\n",
      "       'fffdcd42312777.jpg'], dtype='<U18'), 'val_embeddings': array([[ 0.00413683,  0.02767905, -0.01393281, ..., -0.01033663,\n",
      "        -0.00146162, -0.05184511],\n",
      "       [ 0.07344344,  0.01361496,  0.09238227, ...,  0.0774626 ,\n",
      "        -0.00136799,  0.0222471 ],\n",
      "       [-0.00451848,  0.03621241,  0.00104501, ..., -0.0351309 ,\n",
      "        -0.01341188,  0.0640635 ],\n",
      "       ...,\n",
      "       [-0.08021195, -0.02409246, -0.04801835, ...,  0.03379721,\n",
      "         0.09349999, -0.02171359],\n",
      "       [-0.00514861,  0.00710393,  0.01915118, ..., -0.12107179,\n",
      "        -0.02521873, -0.01924754],\n",
      "       [-0.06452394, -0.01254737,  0.02399073, ...,  0.06170426,\n",
      "        -0.00691608, -0.0745508 ]], dtype=float32), 'val_targets': array(['5015b9e45c33', '43a1413b3da1', '992145e63e59', ...,\n",
      "       '1184686361b3', '5401612696b9', '4ddb2eeb5efb'], dtype=object)}, 'test': {'test_image_names': array(['000110707af0ba.jpg', '0006287ec424cb.jpg', '000809ecb2ccad.jpg',\n",
      "       ..., 'fff96371332c16.jpg', 'fffc1c4d3eabc7.jpg',\n",
      "       'fffc50be10c175.jpg'], dtype='<U18'), 'test_embeddings': array([[-0.03381971,  0.06319313, -0.0086971 , ..., -0.08262127,\n",
      "         0.02683712, -0.05116227],\n",
      "       [ 0.03971311,  0.02479924, -0.00404301, ...,  0.04648989,\n",
      "        -0.07658558,  0.00845479],\n",
      "       [-0.03649535, -0.02673879, -0.03262429, ..., -0.00579198,\n",
      "        -0.04364894,  0.1080614 ],\n",
      "       ...,\n",
      "       [-0.02108362,  0.08786089,  0.0167187 , ..., -0.03405489,\n",
      "        -0.01300064,  0.07055949],\n",
      "       [ 0.04611769, -0.01443468,  0.08624631, ..., -0.07116916,\n",
      "        -0.006709  ,  0.04311379],\n",
      "       [ 0.06774348,  0.07673687,  0.02933813, ..., -0.00277713,\n",
      "        -0.03889615,  0.00950496]], dtype=float32), 'test_targets': array(['0013f1f5f2f0', '0013f1f5f2f0', '0013f1f5f2f0', ...,\n",
      "       '0013f1f5f2f0', '0013f1f5f2f0', '0013f1f5f2f0'], dtype=object)}}\n",
      "convnext:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "convnext:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "convnext_data = torch.load(\"./cache/convnext/convnext_large_384_in22ft1k_384_4.pth\")\n",
    "print(\"convnext: \", convnext_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_data['train']['train_targets'])\n",
    "print(\"convnext: \", convnext_data)\n",
    "\n",
    "convnext_base_data = torch.load(\"./cache/convnext/convnext_base_384_in22ft1k_512_4.pth\")\n",
    "print(\"convnext: \", convnext_base_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_base_data['train']['train_targets'])\n",
    "\n",
    "\n",
    "efficientnet_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b7_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_targets'])\n",
    "\n",
    "efficientnet_b5_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b5_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_targets'])\n",
    "\n",
    "efficientnet_b6_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b6_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad538ff-3150-4bd2-957d-fa4b83e6b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434bf5fbeb1f4955a4e8ffc753a98ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b616773297fc4ba2bb0ab7f940be988e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/170315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c893186e01402e8fb67a90ef705db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8216235718580877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf821d63c17545f7a71c4056303bc964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/170315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4955d7e2fe46424bb91499d72cb06aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  27eed76d08871e.jpg  62ec6fea7ad5 new_individual 9ba53b18ffea 76f62...\n",
      "1  10b11b3fe7cdef.jpg  938b7e931166 new_individual 938b7e931166 5bf17...\n",
      "2  05b9a41635a275.jpg  ca69a5d7c122 new_individual 9bb90a97f325 0d73b...\n",
      "3  28c85429214706.jpg  938b7e931166 new_individual 938b7e931166 5bf17...\n",
      "4  2d6c2725c91aed.jpg  783f357cf2f9 new_individual 2280b5fcc6c2 444dd...\n"
     ]
    }
   ],
   "source": [
    "train_image_names = convnext_data['train']['train_image_names']\n",
    "val_image_names = convnext_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = convnext_data['train']['train_embeddings']\n",
    "val_embeddings = convnext_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = convnext_data['train']['train_targets']\n",
    "val_targets = convnext_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "cv = get_cv(val_targets_df, all_pred)\n",
    "print(cv)\n",
    "\n",
    "\n",
    "predictions = create_predictions_df(val_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "predictions = predictions.drop_duplicates()\n",
    "predictions.to_csv(f\"./cache/convnext/convnext_large_384_in22ft1k_384_0_{cv:.4f}_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3504a8b2-932b-4132-9591-81b2b0961183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f65694ed2604ca6b69bd0d767b4a2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81e91d5fe74499a893e676885384792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/157547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71f0547bdd44e0fb55c24a7b51e6d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8203812845802261\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86d6e9ff60748d9ba342ff00fcdd451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/157547 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8bdb9467d548ef9987d0ab2b1d130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  fbb4bd3b4fa175.jpg  938b7e931166 new_individual 938b7e931166 5bf17...\n",
      "1  eba89d4f35d15c.jpg  19fbb960f07d new_individual 938b7e931166 5bf17...\n",
      "2  eae5feb67dfe9f.jpg  37c7aba965a5 new_individual 938b7e931166 5bf17...\n",
      "3  fc49ab1c7ce8f9.jpg  938b7e931166 new_individual 938b7e931166 5bf17...\n",
      "4  e2284759a75a9e.jpg  37c7aba965a5 new_individual 938b7e931166 5bf17...\n"
     ]
    }
   ],
   "source": [
    "train_image_names = convnext_base_data['train']['train_image_names']\n",
    "val_image_names = convnext_base_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = convnext_base_data['train']['train_embeddings']\n",
    "val_embeddings = convnext_base_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = convnext_base_data['train']['train_targets']\n",
    "val_targets = convnext_base_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "cv = get_cv(val_targets_df, all_pred)\n",
    "print(cv)\n",
    "\n",
    "\n",
    "predictions = create_predictions_df(val_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "predictions = predictions.drop_duplicates()\n",
    "predictions.to_csv(f\"./cache/convnext/convnext_base_384_in22ft1k_512_4_{cv:.4f}_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cfe44-4855-48ae-bca0-59d70f49b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_names = efficientnet_data['train']['train_image_names']\n",
    "val_image_names = efficientnet_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = efficientnet_data['train']['train_embeddings']\n",
    "val_embeddings = efficientnet_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = efficientnet_data['train']['train_targets']\n",
    "val_targets = efficientnet_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "cv = get_cv(val_targets_df, all_pred)\n",
    "print(cv)\n",
    "\n",
    "predictions = create_predictions_df(val_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "predictions = predictions.drop_duplicates()\n",
    "predictions.to_csv(f\"./cache/efficientnet/tf_efficientnet_b7_ns_512_0_{cv:.4f}_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38f4f505-de0a-4254-ba24-30ed01759f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38b4e4209114d84a3d2c4dda99a691d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18bf738a0404d64a617a45858d20fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/165764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d05d2aa650140688e0c186656f23cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8371236720785727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59f919d18044957ab34905904cf95d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/165764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92a98b6d1d74793bcb133c2af1bf90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  27eed76d08871e.jpg  62ec6fea7ad5 new_individual 0dbe0c61bb2f bf2e0...\n",
      "1  05b9a41635a275.jpg  ca69a5d7c122 new_individual 435526bea29f 73549...\n",
      "2  18b27f84a0ab8f.jpg  bbeac4b2964e new_individual 197df8b4dfd4 938b7...\n",
      "3  0b84e4c05bb67a.jpg  ce6e37904aa4 new_individual 938b7e931166 5bf17...\n",
      "4  1b3495ba16b398.jpg  bbeac4b2964e new_individual 197df8b4dfd4 b61c1...\n"
     ]
    }
   ],
   "source": [
    "train_image_names = efficientnet_b5_data['train']['train_image_names']\n",
    "val_image_names = efficientnet_b5_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = efficientnet_b5_data['train']['train_embeddings']\n",
    "val_embeddings = efficientnet_b5_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = efficientnet_b5_data['train']['train_targets']\n",
    "val_targets = efficientnet_b5_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "cv = get_cv(val_targets_df, all_pred)\n",
    "print(cv)\n",
    "\n",
    "predictions = create_predictions_df(val_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "predictions = predictions.drop_duplicates()\n",
    "predictions.to_csv(f\"./cache/efficientnet/tf_efficientnet_b5_ns_512_0_{cv:.4f}_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50bcba06-4ecf-4f26-8077-ab77043c6d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bc314aedc24fcf8c453aa494926471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51b7297da9340188120abf6e4f1f2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/158604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077db1d75c8f4c8e9a09981dfea95aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.841650629460348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e51fb888b70431da29c4fad21056d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/158604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751965d49b2948c0944f130dd421a8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  d2ae3b7448d1d4.jpg  1a20c92ffe68 new_individual 8ee65de044e7 078dd...\n",
      "1  e9d46ff35fe37f.jpg  1a20c92ffe68 new_individual 8ee65de044e7 078dd...\n",
      "2  b43b6b20162cc7.jpg  bbeac4b2964e new_individual 83afdba5e8e5 1d4dc...\n",
      "3  c64ffcd14442e7.jpg  1a20c92ffe68 new_individual 8ee65de044e7 078dd...\n",
      "4  dd4a36261a3c97.jpg  dad9b2cc8452 new_individual a0eabaf2a660 64e52...\n"
     ]
    }
   ],
   "source": [
    "train_image_names = efficientnet_b6_data['train']['train_image_names']\n",
    "val_image_names = efficientnet_b6_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = efficientnet_b6_data['train']['train_embeddings']\n",
    "val_embeddings = efficientnet_b6_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = efficientnet_b6_data['train']['train_targets']\n",
    "val_targets = efficientnet_b6_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "cv = get_cv(val_targets_df, all_pred)\n",
    "print(cv)\n",
    "\n",
    "predictions = create_predictions_df(val_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "predictions = predictions.drop_duplicates()\n",
    "predictions.to_csv(f\"./cache/efficientnet/tf_efficientnet_b6_ns_512_4_{cv:.4f}_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dac2ea6b-93b4-4386-851c-ced84133cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "# sub_files = [\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_0_submission.csv',\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_1_submission.csv',\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_2_submission.csv',\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_3_submission.csv',\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_4_submission.csv'\n",
    "# ]\n",
    "\n",
    "# sub_files = [\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_0_submission.csv',\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_1_submission.csv',\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_2_submission.csv',\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_3_submission.csv',\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_4_submission.csv'\n",
    "# ]\n",
    "\n",
    "# sub_files = [\n",
    "#                  './pl_tf_efficientnet_b7_ns_512_ensemble_submission.csv',\n",
    "#                  './pl_convnext_large_384_in22ft1k_384_ensemble_submission.csv',\n",
    "#                  './swin_tf_764.csv',\n",
    "# ]\n",
    "\n",
    "sub_files = [\n",
    "                 './cache/convnext/convnext_large_384_in22ft1k_384_0_0.8216_submission.csv',\n",
    "                 './cache/efficientnet/tf_efficientnet_b5_ns_512_0_0.8371_submission.csv',\n",
    "                 './cache/efficientnet/tf_efficientnet_b6_ns_512_0_0.8428_submission.csv',\n",
    "                 './cache/efficientnet/tf_efficientnet_b7_ns_512_0_0.8414_submission.csv',\n",
    "]\n",
    "\n",
    "\n",
    "# Weights of the individual subs\n",
    "# sub_weight = [\n",
    "#                 0.748**2,\n",
    "#                 0.742**2,\n",
    "#                 0.764**2,\n",
    "#             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3058b-f132-463a-a72e-a2d5087f359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_b6_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b6_ns_512_0.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_targets'])\n",
    "\n",
    "\n",
    "train_image_names = efficientnet_b6_data['train']['train_image_names']\n",
    "val_image_names = efficientnet_b6_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings = efficientnet_b6_data['train']['train_embeddings']\n",
    "val_embeddings = efficientnet_b6_data['val']['val_embeddings']\n",
    "\n",
    "train_targets = efficientnet_b6_data['train']['train_targets']\n",
    "val_targets = efficientnet_b6_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "best_cv = 0\n",
    "for w1 in range(0, 110, 10):\n",
    "    for w2 in range(0, 110-w1, 10):\n",
    "        for w3 in range(0, 110-w1-w2, 10):\n",
    "            w4 = 110-w1-w2-w3 \n",
    "            sub_weight = [w1/100, w2/100, w3/100, w4/100]\n",
    "    # w2 = 105-w1\n",
    "    # sub_weight = [w1/100, w2/100]\n",
    "            Hlabel = 'image' \n",
    "            Htarget = 'predictions'\n",
    "            npt = 6\n",
    "            place_weights = {}\n",
    "            for i in range(npt):\n",
    "                place_weights[i] = (1 / (i + 1))\n",
    "\n",
    "            print(place_weights)\n",
    "\n",
    "            lg = len(sub_files)\n",
    "            sub = [None]*lg\n",
    "            for i, file in enumerate( sub_files ):   \n",
    "                # print(\"Reading {}: w={} - {}\". format(i, sub_weight[i], file))\n",
    "                reader = csv.DictReader(open(file,\"r\"))\n",
    "                sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))\n",
    "\n",
    "            # out = open(\"pl_convnext_large_384_in22ft1k_384_ensemble_submission.csv\", \"w\", newline='')\n",
    "            # out = open(\"pl_ensemble_v20220409_submission.csv\", \"w\", newline='')\n",
    "            # writer = csv.writer(out)\n",
    "            # writer.writerow([Hlabel,Htarget])\n",
    "\n",
    "            all_pred = {}\n",
    "\n",
    "            for p, row in enumerate(sub[0]):\n",
    "                target_weight = {}\n",
    "                for s in range(lg):\n",
    "                    row1 = sub[s][p]\n",
    "                    for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "                        target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "                tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "                all_pred[row1[Hlabel]] = tops_trgt\n",
    "                # writer.writerow([row1[Hlabel], \" \".join(tops_trgt)])\n",
    "            # out.close()\n",
    "            cv = get_cv(val_targets_df, all_pred)\n",
    "            print(f\"w1: {w1}, w2: {w2}, w3: {w3}, w4: {w4}, cv: {cv}\")\n",
    "            if cv > best_cv:\n",
    "                best_cv = cv\n",
    "        \n",
    "print(f\"best_cv: {best_cv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934f810-2262-4b46-8d28-1c76ab55a25c",
   "metadata": {},
   "source": [
    "# Embedding Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14d7d21a-f48a-4e27-a414-447551c14b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "convnext:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "(33252,)\n",
      "convnext:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "convnext:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "(33252,)\n",
      "(8314,)\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "(33252,)\n",
      "(8314,)\n",
      "efficientnet:  ['00021adfb725ed.jpg' '000562241d384d.jpg' '0007c33415ce37.jpg' ...\n",
      " 'ffae18d2939ffc.jpg' 'ffaf6c062ccdef.jpg' 'ffb1ddd9a59530.jpg']\n",
      "efficientnet:  ['cadddb1636b9' '1a71fbb72250' '60008f293a2b' ... '1a71fbb72250'\n",
      " '938167f9ea20' 'f92fcbf8f42d']\n",
      "(33252,)\n",
      "(8314,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "convnext_data = torch.load(\"./cache/convnext/convnext_large_384_in22ft1k_384_4.pth\")\n",
    "print(\"convnext: \", convnext_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_data['train']['train_targets'])\n",
    "print(convnext_data['train']['train_image_names'].shape)\n",
    "\n",
    "convnext_base_data = torch.load(\"./cache/convnext/convnext_base_384_in22ft1k_512_4.pth\")\n",
    "print(\"convnext: \", convnext_base_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_base_data['train']['train_targets'])\n",
    "\n",
    "\n",
    "efficientnet_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b7_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_targets'])\n",
    "print(efficientnet_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_data['val']['val_image_names'].shape)\n",
    "\n",
    "\n",
    "efficientnet_b5_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b5_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_targets'])\n",
    "print(efficientnet_b5_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_b5_data['val']['val_image_names'].shape)\n",
    "\n",
    "efficientnet_b6_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b6_ns_512_4.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_targets'])\n",
    "print(efficientnet_b6_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_b6_data['val']['val_image_names'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2a9b2cf-7e3a-4e65-a944-7fc3f64a8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335be1e184e04411a8cce649e5444938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating val_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d9531731f0454ebf1da79d4c5f8166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.4:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da28923cf81a4d86b672b2fd5c155b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.4 cv=0.8626172720712052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4262068ef94c5595c3d7f3626815a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.42:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a31c27d2be4be6aacc66a2ecab463f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.42 cv=0.8671878758720231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bbb7c8b1fc4089a7755d4a04bdf30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.44:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d97d82c5109479e9205ac22021ec4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.44 cv=0.8675487130141929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102b546f36cd4065afb30e0bf8840949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.46:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f765f0bb0b8a4e1fb2eb18015c8c8e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.46 cv=0.865323550637479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46d1752f52946d4916cf4758637fbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.48:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca7d77c643b4aa287fd6941f34413c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.48 cv=0.8618955977868655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974897ec156043f2aa8d0cd40df5234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.5:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e035e79893ba4952ba4cece761e2c59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.5 cv=0.8580466682703873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608ae7f254114d35a6a0210a35cf10c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.52:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd58ba15cd834e9d941d44ee3c296176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.52 cv=0.85227327399567\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b1f573cea8420a94f8558383f84f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.54:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6ad671692b47a185a1918c5d2a1446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.54 cv=0.8473418330526823\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848f2e2f517c40caae11da23a2cfb04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.56:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e24b9ef8e743e3a754d8182fbcb103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.56 cv=0.8410271830647101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f98db48f54b0e9b38d14866483106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.58:   0%|          | 0/159187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98454a4751904939a11d133a51b8f1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th=0.58 cv=0.8359153235506375\n",
      "best_th=0.44\n",
      "best_cv=0.8675487130141929\n",
      "best_th_adjusted=0.4\n"
     ]
    }
   ],
   "source": [
    "NORM = True\n",
    "\n",
    "train_image_names = convnext_data['train']['train_image_names']\n",
    "val_image_names = convnext_data['val']['val_image_names']\n",
    "\n",
    "\n",
    "train_embeddings_conv = convnext_data['train']['train_embeddings']\n",
    "val_embeddings_conv = convnext_data['val']['val_embeddings']\n",
    "\n",
    "train_embeddings_conv_base = convnext_base_data['train']['train_embeddings']\n",
    "val_embeddings_conv_base = convnext_base_data['val']['val_embeddings']\n",
    "\n",
    "train_embeddings_eff = efficientnet_data['train']['train_embeddings']\n",
    "val_embeddings_eff = efficientnet_data['val']['val_embeddings']\n",
    "\n",
    "train_embeddings_eff_b5 = efficientnet_b5_data['train']['train_embeddings']\n",
    "val_embeddings_eff_b5 = efficientnet_b5_data['val']['val_embeddings']\n",
    "\n",
    "train_embeddings_eff_b6 = efficientnet_b6_data['train']['train_embeddings']\n",
    "val_embeddings_eff_b6 = efficientnet_b6_data['val']['val_embeddings']\n",
    "\n",
    "if NORM:\n",
    "    train_embeddings_conv = train_embeddings_conv / np.linalg.norm(train_embeddings_conv, axis=1)[:, None]\n",
    "    train_embeddings_conv_base = train_embeddings_conv_base / np.linalg.norm(train_embeddings_conv_base, axis=1)[:, None]\n",
    "    train_embeddings_eff = train_embeddings_eff / np.linalg.norm(train_embeddings_eff, axis=1)[:, None]\n",
    "    train_embeddings_eff_b5 = train_embeddings_eff_b5 / np.linalg.norm(train_embeddings_eff_b5, axis=1)[:, None]\n",
    "    train_embeddings_eff_b6 = train_embeddings_eff_b6 / np.linalg.norm(train_embeddings_eff_b6, axis=1)[:, None]\n",
    "    val_embeddings_conv = val_embeddings_conv / np.linalg.norm(val_embeddings_conv, axis=1)[:, None]\n",
    "    val_embeddings_conv_base = val_embeddings_conv_base / np.linalg.norm(val_embeddings_conv_base, axis=1)[:, None]\n",
    "    val_embeddings_eff = val_embeddings_eff / np.linalg.norm(val_embeddings_eff, axis=1)[:, None]\n",
    "    val_embeddings_eff_b5 = val_embeddings_eff_b5 / np.linalg.norm(val_embeddings_eff_b5, axis=1)[:, None]\n",
    "    val_embeddings_eff_b6 = val_embeddings_eff_b6 / np.linalg.norm(val_embeddings_eff_b6, axis=1)[:, None]\n",
    "\n",
    "train_concat_list = [train_embeddings_conv, train_embeddings_conv_base, train_embeddings_eff, train_embeddings_eff_b5, train_embeddings_eff_b6]\n",
    "val_concat_list = [val_embeddings_conv, val_embeddings_conv_base, val_embeddings_eff, val_embeddings_eff_b5, val_embeddings_eff_b6]\n",
    "train_embeddings = np.concatenate(train_concat_list, axis=1)\n",
    "val_embeddings = np.concatenate(val_concat_list, axis=1)\n",
    "\n",
    "if NORM:\n",
    "    train_embeddings = train_embeddings / np.linalg.norm(train_embeddings, axis=1)[:, None]\n",
    "    val_embeddings = val_embeddings / np.linalg.norm(val_embeddings, axis=1)[:, None]\n",
    "\n",
    "train_targets = efficientnet_data['train']['train_targets']\n",
    "val_targets = efficientnet_data['val']['val_targets']\n",
    "\n",
    "D, I = create_and_search_index(512*len(train_concat_list), train_embeddings, val_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "val_targets_df = create_val_targets_df(train_targets, val_image_names, val_targets)\n",
    "val_df = create_distances_df(val_image_names, train_targets, D, I, \"val\")\n",
    "# all_pred =  get_predictions(val_df, threshold=0.5)\n",
    "\n",
    "# cv = get_cv(val_targets_df, all_pred)\n",
    "# print(cv)\n",
    "best_th, best_cv = get_best_threshold(val_targets_df, val_df)\n",
    "\n",
    "# predictions = create_predictions_df(val_df, 0.5)\n",
    "# print(f\"predictions.head()={predictions.head()}\")\n",
    "# predictions = predictions.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973819f-5c16-4865-8c39-da9f6db14366",
   "metadata": {},
   "source": [
    "# Embedding Ensemble Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2ffaa0e-7b76-42d3-bbf6-638fd2c6ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext:  ['000562241d384d.jpg' '001b0900f56e89.jpg' '0029d877c29ab4.jpg' ...\n",
      " 'ffecd4a6afc94a.jpg' 'ffeed309a3081e.jpg' 'fff219829b3c68.jpg']\n",
      "convnext:  ['1a71fbb72250' 'bc14b5054353' 'c27f0a7f4e5a' ... '424d49bef76e'\n",
      " 'afe8b7c1b1c6' 'ada9e2afe2bd']\n",
      "(33252,)\n",
      "convnext:  ['000562241d384d.jpg' '001b0900f56e89.jpg' '0029d877c29ab4.jpg' ...\n",
      " 'ffecd4a6afc94a.jpg' 'ffeed309a3081e.jpg' 'fff219829b3c68.jpg']\n",
      "convnext:  ['1a71fbb72250' 'bc14b5054353' 'c27f0a7f4e5a' ... '424d49bef76e'\n",
      " 'afe8b7c1b1c6' 'ada9e2afe2bd']\n",
      "efficientnet:  ['000562241d384d.jpg' '001b0900f56e89.jpg' '0029d877c29ab4.jpg' ...\n",
      " 'ffecd4a6afc94a.jpg' 'ffeed309a3081e.jpg' 'fff219829b3c68.jpg']\n",
      "efficientnet:  ['1a71fbb72250' 'bc14b5054353' 'c27f0a7f4e5a' ... '424d49bef76e'\n",
      " 'afe8b7c1b1c6' 'ada9e2afe2bd']\n",
      "(33252,)\n",
      "(8315,)\n",
      "efficientnet:  ['000562241d384d.jpg' '001b0900f56e89.jpg' '0029d877c29ab4.jpg' ...\n",
      " 'ffecd4a6afc94a.jpg' 'ffeed309a3081e.jpg' 'fff219829b3c68.jpg']\n",
      "efficientnet:  ['1a71fbb72250' 'bc14b5054353' 'c27f0a7f4e5a' ... '424d49bef76e'\n",
      " 'afe8b7c1b1c6' 'ada9e2afe2bd']\n",
      "(33252,)\n",
      "(8315,)\n",
      "efficientnet:  ['000562241d384d.jpg' '001b0900f56e89.jpg' '0029d877c29ab4.jpg' ...\n",
      " 'ffecd4a6afc94a.jpg' 'ffeed309a3081e.jpg' 'fff219829b3c68.jpg']\n",
      "efficientnet:  ['1a71fbb72250' 'bc14b5054353' 'c27f0a7f4e5a' ... '424d49bef76e'\n",
      " 'afe8b7c1b1c6' 'ada9e2afe2bd']\n",
      "(33252,)\n",
      "(8315,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "convnext_data = torch.load(\"./cache/convnext/convnext_large_384_in22ft1k_384_0.pth\")\n",
    "print(\"convnext: \", convnext_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_data['train']['train_targets'])\n",
    "print(convnext_data['train']['train_image_names'].shape)\n",
    "\n",
    "convnext_base_data = torch.load(\"./cache/convnext/convnext_base_384_in22ft1k_512_0.pth\")\n",
    "print(\"convnext: \", convnext_base_data['train']['train_image_names'])\n",
    "print(\"convnext: \", convnext_base_data['train']['train_targets'])\n",
    "\n",
    "\n",
    "efficientnet_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b7_ns_512_0.pth\")\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_data['train']['train_targets'])\n",
    "print(efficientnet_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_data['val']['val_image_names'].shape)\n",
    "\n",
    "\n",
    "efficientnet_b5_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b5_ns_512_0.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b5_data['train']['train_targets'])\n",
    "print(efficientnet_b5_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_b5_data['val']['val_image_names'].shape)\n",
    "\n",
    "efficientnet_b6_data = torch.load(\"./cache/efficientnet/tf_efficientnet_b6_ns_512_0.pth\")\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_image_names'])\n",
    "print(\"efficientnet: \", efficientnet_b6_data['train']['train_targets'])\n",
    "print(efficientnet_b6_data['train']['train_image_names'].shape)\n",
    "print(efficientnet_b6_data['val']['val_image_names'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0193c6c-a45c-4a60-a6d4-a1e1a74ebb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated train_embeddings and train_targets with val data\n",
      "Created index with train_embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd65a8e4c3e4f6eb5c4d864afbec628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating test_df: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df=\n",
      "                image        target  distances\n",
      "0  a3a9c424ef9f06.jpg  0ed88187dcb5   0.993922\n",
      "1  e9abb76a5bed89.jpg  35f898e6595e   0.993084\n",
      "2  5fb61ff7e07076.jpg  2e0b381d3467   0.992452\n",
      "3  7a785b700b0339.jpg  c93996835aa8   0.991386\n",
      "4  0d61c514065cef.jpg  2dd8974deb39   0.991109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f08d79ebf94ae3893180214c27fc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating predictions for threshold=0.4:   0%|          | 0/644752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85eb2c1588c240eba1f8e85a6b1e88ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions.head()=                image                                        predictions\n",
      "0  a3a9c424ef9f06.jpg  0ed88187dcb5 new_individual 3608e0cc1c01 b9a19...\n",
      "1  e9abb76a5bed89.jpg  35f898e6595e new_individual 938b7e931166 5bf17...\n",
      "2  5fb61ff7e07076.jpg  2e0b381d3467 new_individual 938b7e931166 5bf17...\n",
      "3  7a785b700b0339.jpg  c93996835aa8 new_individual 958e201f3f4d 862ad...\n",
      "4  0d61c514065cef.jpg  2dd8974deb39 new_individual c77668767d5c 7aa63...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_DIR = Path('../datasets/kaggle/happy-whale-and-dolphin/')\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "\n",
    "DATA_ROOT_DIR = INPUT_DIR / \"happy-whale-and-dolphin-backfin\"\n",
    "TRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\n",
    "TEST_DIR = DATA_ROOT_DIR / \"test_images\"\n",
    "\n",
    "PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"720\" / \"submission.csv\"\n",
    "IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"backfin\" / \"ids_without_backfin.npy\"\n",
    "\n",
    "NORM = True\n",
    "\n",
    "train_image_names = convnext_data['train']['train_image_names']\n",
    "val_image_names = convnext_data['val']['val_image_names']\n",
    "test_image_names = convnext_data['test']['test_image_names']\n",
    "\n",
    "\n",
    "train_embeddings_conv = convnext_data['train']['train_embeddings']\n",
    "val_embeddings_conv = convnext_data['val']['val_embeddings']\n",
    "test_embeddings_conv = convnext_data['test']['test_embeddings']\n",
    "\n",
    "train_embeddings_conv_base = convnext_base_data['train']['train_embeddings']\n",
    "val_embeddings_conv_base = convnext_base_data['val']['val_embeddings']\n",
    "test_embeddings_conv_base = convnext_base_data['test']['test_embeddings']\n",
    "\n",
    "train_embeddings_eff = efficientnet_data['train']['train_embeddings']\n",
    "val_embeddings_eff = efficientnet_data['val']['val_embeddings']\n",
    "test_embeddings_eff = efficientnet_data['test']['test_embeddings']\n",
    "\n",
    "train_embeddings_eff_b5 = efficientnet_b5_data['train']['train_embeddings']\n",
    "val_embeddings_eff_b5 = efficientnet_b5_data['val']['val_embeddings']\n",
    "test_embeddings_eff_b5 = efficientnet_b5_data['test']['test_embeddings']\n",
    "\n",
    "train_embeddings_eff_b6 = efficientnet_b6_data['train']['train_embeddings']\n",
    "val_embeddings_eff_b6 = efficientnet_b6_data['val']['val_embeddings']\n",
    "test_embeddings_eff_b6 = efficientnet_b6_data['test']['test_embeddings']\n",
    "\n",
    "\n",
    "if NORM:\n",
    "    train_embeddings_conv = train_embeddings_conv / np.linalg.norm(train_embeddings_conv, axis=1)[:, None]\n",
    "    train_embeddings_conv_base = train_embeddings_conv_base / np.linalg.norm(train_embeddings_conv_base, axis=1)[:, None]\n",
    "    train_embeddings_eff = train_embeddings_eff / np.linalg.norm(train_embeddings_eff, axis=1)[:, None]\n",
    "    train_embeddings_eff_b5 = train_embeddings_eff_b5 / np.linalg.norm(train_embeddings_eff_b5, axis=1)[:, None]\n",
    "    train_embeddings_eff_b6 = train_embeddings_eff_b6 / np.linalg.norm(train_embeddings_eff_b6, axis=1)[:, None]\n",
    "    val_embeddings_conv = val_embeddings_conv / np.linalg.norm(val_embeddings_conv, axis=1)[:, None]\n",
    "    val_embeddings_conv_base = val_embeddings_conv_base / np.linalg.norm(val_embeddings_conv_base, axis=1)[:, None]\n",
    "    val_embeddings_eff = val_embeddings_eff / np.linalg.norm(val_embeddings_eff, axis=1)[:, None]\n",
    "    val_embeddings_eff_b5 = val_embeddings_eff_b5 / np.linalg.norm(val_embeddings_eff_b5, axis=1)[:, None]\n",
    "    val_embeddings_eff_b6 = val_embeddings_eff_b6 / np.linalg.norm(val_embeddings_eff_b6, axis=1)[:, None]\n",
    "    test_embeddings_conv = test_embeddings_conv / np.linalg.norm(test_embeddings_conv, axis=1)[:, None]\n",
    "    test_embeddings_conv_base = test_embeddings_conv_base / np.linalg.norm(test_embeddings_conv_base, axis=1)[:, None]\n",
    "    test_embeddings_eff = test_embeddings_eff / np.linalg.norm(test_embeddings_eff, axis=1)[:, None]\n",
    "    test_embeddings_eff_b5 = test_embeddings_eff_b5 / np.linalg.norm(test_embeddings_eff_b5, axis=1)[:, None]\n",
    "    test_embeddings_eff_b6 = test_embeddings_eff_b6 / np.linalg.norm(test_embeddings_eff_b6, axis=1)[:, None]\n",
    "\n",
    "    \n",
    "train_concat_list = [train_embeddings_conv, train_embeddings_conv_base, train_embeddings_eff, train_embeddings_eff_b5, train_embeddings_eff_b6]\n",
    "val_concat_list = [val_embeddings_conv, val_embeddings_conv_base, val_embeddings_eff, val_embeddings_eff_b5, val_embeddings_eff_b6]\n",
    "test_concat_list = [test_embeddings_conv, test_embeddings_conv_base, test_embeddings_eff, test_embeddings_eff_b5, test_embeddings_eff_b6]\n",
    "\n",
    "train_embeddings = np.concatenate(train_concat_list, axis=1)\n",
    "val_embeddings = np.concatenate(val_concat_list, axis=1)\n",
    "test_embeddings = np.concatenate(test_concat_list, axis=1)\n",
    "\n",
    "if NORM:\n",
    "    train_embeddings = train_embeddings / np.linalg.norm(train_embeddings, axis=1)[:, None]\n",
    "    val_embeddings = val_embeddings / np.linalg.norm(val_embeddings, axis=1)[:, None]\n",
    "    test_embeddings = test_embeddings / np.linalg.norm(test_embeddings, axis=1)[:, None]\n",
    "\n",
    "train_targets = convnext_data['train']['train_targets']\n",
    "val_targets = convnext_data['val']['val_targets']\n",
    "    \n",
    "train_embeddings = np.concatenate([train_embeddings, val_embeddings])\n",
    "train_targets = np.concatenate([train_targets, val_targets])\n",
    "print(\"Updated train_embeddings and train_targets with val data\")\n",
    "\n",
    "D, I = create_and_search_index(512 * len(train_concat_list), train_embeddings, test_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n",
    "print(f\"test_df=\\n{test_df.head()}\")\n",
    "\n",
    "predictions = create_predictions_df(test_df, 0.4)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "\n",
    "# Fix missing predictions\n",
    "# From https://www.kaggle.com/code/jpbremer/backfins-arcface-tpu-effnet/notebook\n",
    "public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n",
    "ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True)\n",
    "\n",
    "ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n",
    "\n",
    "predictions = pd.concat(\n",
    "[\n",
    "    predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n",
    "    public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n",
    "    public_predictions[public_predictions[\"image\"].isin(ids2)],\n",
    "]\n",
    ")\n",
    "predictions = predictions.drop_duplicates()\n",
    "\n",
    "predictions.to_csv(\"embd_convnext_large_384_in22ft1k_384_0+convnext_base_384_in22ft1k_512_0+tf_efficientnet_b7_ns_512_0+tf_efficientnet_b5_ns_512_0+tf_efficientnet_b6_ns_512_0_norm_submission.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b216f-0300-47a1-9910-cda267c79b47",
   "metadata": {},
   "source": [
    "# 加权测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ee1f6-c5f0-460d-a79c-e95da5f3849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "sub_files = [\n",
    "                 './pl_convnext_large_384_in22ft1k_384_0_submission.csv',\n",
    "                 './pl_tf_efficientnet_b7_ns_512_0_submission.csv',\n",
    "]\n",
    "\n",
    "# Weights of the individual subs\n",
    "sub_weight = [\n",
    "                0.4,\n",
    "                0.6\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b72ed-e291-4211-9eca-599ee710854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hlabel = 'image' \n",
    "Htarget = 'predictions'\n",
    "npt = 6\n",
    "place_weights = {}\n",
    "for i in range(npt):\n",
    "    place_weights[i] = ( 1 / (i + 1) )\n",
    "    \n",
    "print(place_weights)\n",
    "\n",
    "lg = len(sub_files)\n",
    "sub = [None]*lg\n",
    "for i, file in enumerate( sub_files ):\n",
    "   \n",
    "    print(\"Reading {}: w={} - {}\". format(i, sub_weight[i], file))\n",
    "    reader = csv.DictReader(open(file,\"r\"))\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))\n",
    "\n",
    "out = open(\"convnext_large_384_in22ft1k_384_1+tf_efficientnet_b7_ns_512_1_submission.csv\", \"w\", newline='')\n",
    "writer = csv.writer(out)\n",
    "writer.writerow([Hlabel,Htarget])\n",
    "\n",
    "for p, row in enumerate(sub[0]):\n",
    "    target_weight = {}\n",
    "    for s in range(lg):\n",
    "        row1 = sub[s][p]\n",
    "        for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "            target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "    tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "    writer.writerow([row1[Hlabel], \" \".join(tops_trgt)])\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4e601-0767-4043-b0d2-dede2bcafffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = Path('../datasets/kaggle/happy-whale-and-dolphin/')\n",
    "OUTPUT_DIR = Path(\"./\")\n",
    "\n",
    "DATA_ROOT_DIR = INPUT_DIR / \"happy-whale-and-dolphin-backfin\"\n",
    "TRAIN_DIR = DATA_ROOT_DIR / \"train_images\"\n",
    "TEST_DIR = DATA_ROOT_DIR / \"test_images\"\n",
    "\n",
    "PUBLIC_SUBMISSION_CSV_PATH = INPUT_DIR / \"720\" / \"submission.csv\"\n",
    "IDS_WITHOUT_BACKFIN_PATH = INPUT_DIR / \"backfin\" / \"ids_without_backfin.npy\"\n",
    "\n",
    "train_image_names = convnext_data['train']['train_image_names']\n",
    "val_image_names = convnext_data['val']['val_image_names']\n",
    "test_image_names = convnext_data['test']['test_image_names']\n",
    "\n",
    "train_embeddings = convnext_data['train']['train_embeddings']\n",
    "val_embeddings = convnext_data['val']['val_embeddings']\n",
    "test_embeddings = convnext_data['test']['test_embeddings']\n",
    "\n",
    "train_targets = convnext_data['train']['train_targets']\n",
    "val_targets = convnext_data['val']['val_targets']\n",
    "test_targets = convnext_data['test']['test_targets']\n",
    "\n",
    "\n",
    "train_embeddings = np.concatenate([train_embeddings, val_embeddings])\n",
    "train_targets = np.concatenate([train_targets, val_targets])\n",
    "print(\"Updated train_embeddings and train_targets with val data\")\n",
    "\n",
    "D, I = create_and_search_index(512, train_embeddings, test_embeddings, 50)  # noqa: E741\n",
    "print(\"Created index with train_embeddings\")\n",
    "\n",
    "test_df = create_distances_df(test_image_names, train_targets, D, I, \"test\")\n",
    "print(f\"test_df=\\n{test_df.head()}\")\n",
    "\n",
    "predictions = create_predictions_df(test_df, 0.5)\n",
    "print(f\"predictions.head()={predictions.head()}\")\n",
    "\n",
    "# Fix missing predictions\n",
    "# From https://www.kaggle.com/code/jpbremer/backfins-arcface-tpu-effnet/notebook\n",
    "public_predictions = pd.read_csv(PUBLIC_SUBMISSION_CSV_PATH)\n",
    "# public_predictions = pd.read_csv(\"./swin_tf_764.csv\")\n",
    "ids_without_backfin = np.load(IDS_WITHOUT_BACKFIN_PATH, allow_pickle=True)\n",
    "\n",
    "ids2 = public_predictions[\"image\"][~public_predictions[\"image\"].isin(predictions[\"image\"])]\n",
    "print(ids2)\n",
    "print(public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)])\n",
    "\n",
    "predictions = pd.concat(\n",
    "    [\n",
    "        predictions[~(predictions[\"image\"].isin(ids_without_backfin))],\n",
    "        public_predictions[public_predictions[\"image\"].isin(ids_without_backfin)],\n",
    "        public_predictions[public_predictions[\"image\"].isin(ids2)],\n",
    "    ]\n",
    ")\n",
    "predictions = predictions.drop_duplicates()\n",
    "\n",
    "# predictions.to_csv(SUBMISSION_CSV_PATH, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf08ba8-d11b-40ac-bfe5-733b3111051f",
   "metadata": {},
   "source": [
    "# 二次 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be23ed93-07d1-4099-beee-8a3c4eab5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "\n",
    "sub_files = [\n",
    "                 './embd_convnext_large_384_in22ft1k_384_0+convnext_base_384_in22ft1k_512_0+tf_efficientnet_b7_ns_512_0+tf_efficientnet_b5_ns_512_0+tf_efficientnet_b6_ns_512_0_norm_submission.csv',\n",
    "                 './embd_convnext_large_384_in22ft1k_384_1+convnext_base_384_in22ft1k_512_1+tf_efficientnet_b7_ns_512_1+tf_efficientnet_b5_ns_512_1+tf_efficientnet_b6_ns_512_1_norm_submission.csv',\n",
    "                 './embd_convnext_large_384_in22ft1k_384_2+convnext_base_384_in22ft1k_512_2+tf_efficientnet_b7_ns_512_2+tf_efficientnet_b5_ns_512_2+tf_efficientnet_b6_ns_512_2_norm_submission.csv',\n",
    "                 './embd_convnext_large_384_in22ft1k_384_3+convnext_base_384_in22ft1k_512_3+tf_efficientnet_b7_ns_512_3+tf_efficientnet_b5_ns_512_3+tf_efficientnet_b6_ns_512_3_norm_submission.csv',\n",
    "                 './embd_convnext_large_384_in22ft1k_384_4+convnext_base_384_in22ft1k_512_4+tf_efficientnet_b7_ns_512_4+tf_efficientnet_b5_ns_512_4+tf_efficientnet_b6_ns_512_4_norm_submission.csv'\n",
    "            ]\n",
    "\n",
    "# sub_files = [    './embd_convnext_large_384_in22ft1k_384+tf_efficientnet_b7_ns_512+tf_efficientnet_b5_ns_512_norm_submission.csv',\n",
    "#                  './tf_swin_effb7_effb5_777.csv',\n",
    "#                  './tf_effv2_762.csv'\n",
    "\n",
    "#             ]\n",
    "\n",
    "# Weights of the individual subs\n",
    "sub_weight = [\n",
    "                 0.2,\n",
    "                 0.2,\n",
    "                 0.2,\n",
    "                 0.2,\n",
    "                 0.2,\n",
    "             ]\n",
    "\n",
    "# Weights of the individual subs\n",
    "# N_wight=0.5\n",
    "# Base_wight=1.5\n",
    "\n",
    "# l1 = [0.774, 0.777, 0.762]\n",
    "# l2 = [Base_wight+N_wight*i for i in range(len(l1))]\n",
    "# sub_weight = list(map(lambda x,y: x**y ,l1,l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2ebea01-3f12-4199-b536-5cfa2ee05dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 0.5, 2: 0.3333333333333333, 3: 0.25, 4: 0.2, 5: 0.16666666666666666}\n",
      "Reading 0: w=0.2 - ./embd_convnext_large_384_in22ft1k_384_0+convnext_base_384_in22ft1k_512_0+tf_efficientnet_b7_ns_512_0+tf_efficientnet_b5_ns_512_0+tf_efficientnet_b6_ns_512_0_norm_submission.csv\n",
      "Reading 1: w=0.2 - ./embd_convnext_large_384_in22ft1k_384_1+convnext_base_384_in22ft1k_512_1+tf_efficientnet_b7_ns_512_1+tf_efficientnet_b5_ns_512_1+tf_efficientnet_b6_ns_512_1_norm_submission.csv\n",
      "Reading 2: w=0.2 - ./embd_convnext_large_384_in22ft1k_384_2+convnext_base_384_in22ft1k_512_2+tf_efficientnet_b7_ns_512_2+tf_efficientnet_b5_ns_512_2+tf_efficientnet_b6_ns_512_2_norm_submission.csv\n",
      "Reading 3: w=0.2 - ./embd_convnext_large_384_in22ft1k_384_3+convnext_base_384_in22ft1k_512_3+tf_efficientnet_b7_ns_512_3+tf_efficientnet_b5_ns_512_3+tf_efficientnet_b6_ns_512_3_norm_submission.csv\n",
      "Reading 4: w=0.2 - ./embd_convnext_large_384_in22ft1k_384_4+convnext_base_384_in22ft1k_512_4+tf_efficientnet_b7_ns_512_4+tf_efficientnet_b5_ns_512_4+tf_efficientnet_b6_ns_512_4_norm_submission.csv\n"
     ]
    }
   ],
   "source": [
    "Hlabel = 'image' \n",
    "Htarget = 'predictions'\n",
    "npt = 6\n",
    "place_weights = {}\n",
    "for i in range(npt):\n",
    "    place_weights[i] = ( 1 / (i + 1) )\n",
    "    \n",
    "print(place_weights)\n",
    "\n",
    "lg = len(sub_files)\n",
    "sub = [None]*lg\n",
    "for i, file in enumerate( sub_files ):\n",
    "   \n",
    "    print(\"Reading {}: w={} - {}\". format(i, sub_weight[i], file))\n",
    "    reader = csv.DictReader(open(file,\"r\"))\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))\n",
    "\n",
    "out = open(\"embd_convnext_large_384_in22ft1k_384+convnext_base_384_in22ft1k_512+tf_efficientnet_b7_ns_512+tf_efficientnet_b5_ns_512+tf_efficientnet_b6_ns_512_norm_submission.csv\", \"w\", newline='')\n",
    "writer = csv.writer(out)\n",
    "writer.writerow([Hlabel,Htarget])\n",
    "\n",
    "for p, row in enumerate(sub[0]):\n",
    "    target_weight = {}\n",
    "    for s in range(lg):\n",
    "        row1 = sub[s][p]\n",
    "        for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "            target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "    tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "    writer.writerow([row1[Hlabel], \" \".join(tops_trgt)])\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518dd4e7-e7a5-494a-9693-eb77d3f5eb50",
   "metadata": {},
   "source": [
    "# Results \n",
    "\n",
    "## CSV ensemble\n",
    "\n",
    "fold4: convnext_large_384_in22ft1k_384_4_0.8233 * 0.35 + tf_efficientnet_b7_ns_512_4_0.8387 * 0.65 = 0.8436\n",
    "\n",
    "fold3: convnext_large_384_in22ft1k_384_3_0.8200 * 0.4 + tf_efficientnet_b7_ns_512_3_0.8397 * 0.6 = 0.8443\n",
    "\n",
    "fold2: convnext_large_384_in22ft1k_384_2_0.8182 * 0.4 + tf_efficientnet_b7_ns_512_2_0.8390 * 0.6 = 0.8445\n",
    "\n",
    "fold1: convnext_large_384_in22ft1k_384_1_0.8169 * 0.4 + tf_efficientnet_b7_ns_512_1_0.8311 * 0.6 = 0.8373\n",
    "\n",
    "fold0: \n",
    "convnext_large_384_in22ft1k_384_0_0.8216 * 0.4 +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 * 0.6 = 0.8463\n",
    "\n",
    "## Emebdding ensemble\n",
    "\n",
    "fold4: \n",
    "\n",
    "convnext_large_384_in22ft1k_384_4_0.8233 + tf_efficientnet_b7_ns_512_4_0.8387\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8559798733060701\n",
    "best_th_adjusted=0.8\n",
    "\n",
    "convnext_large_384_in22ft1k_384_4_0.8233 + \n",
    "tf_efficientnet_b7_ns_512_4_0.8387 + \n",
    "tf_efficientnet_b5_ns_512_4_0.8400 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8645016438136476\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "\n",
    "convnext_large_384_in22ft1k_384_4_0.8233 + \n",
    "tf_efficientnet_b7_ns_512_4_0.8387 + \n",
    "tf_efficientnet_b6_ns_512_4_0.8417 +\n",
    "tf_efficientnet_b5_ns_512_4_0.8400 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8657306073361394\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "tf_efficientnet_b7_ns_512_4_0.8387 + \n",
    "tf_efficientnet_b5_ns_512_4_0.8400\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8587302541897202\n",
    "best_th_adjusted=0.9\n",
    "\n",
    "tf_efficientnet_b7_ns_512_4_0.8387 + \n",
    "tf_efficientnet_b5_ns_512_4_0.8400 + NORM\n",
    "\n",
    "best_th=0.5\n",
    "best_cv=0.8539792318178173\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "\n",
    "convnext_large_384_in22ft1k_384_4_0.8233 + \n",
    "convnext_base_384_in22ft1k_512_4_0.8204 +\n",
    "tf_efficientnet_b7_ns_512_4_0.8387 + \n",
    "tf_efficientnet_b6_ns_512_4_0.8417 +\n",
    "tf_efficientnet_b5_ns_512_4_0.8400 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8675487130141929\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "fold3:\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + tf_efficientnet_b7_ns_512_3_0.8397\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8532491481258769\n",
    "best_th_adjusted=0.8\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + \n",
    "tf_efficientnet_b7_ns_512_3_0.8397 + NORM\n",
    "\n",
    "best_th=0.5\n",
    "best_cv=0.8474764481860092\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b7_ns_512_3_0.8391\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8577330126277812\n",
    "best_th_adjusted=0.9\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + \n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b7_ns_512_3_0.8391\n",
    "\n",
    "best_th=1.0\n",
    "best_cv=0.8112146722790137\n",
    "best_th_adjusted=1.\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + \n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b7_ns_512_3_0.8391 + Norm\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8615453998797354\n",
    "best_th_adjusted=0.42\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + \n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b6_ns_512_3_0.8418 +\n",
    "tf_efficientnet_b7_ns_512_3_0.8391 + Norm\n",
    "\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8657306073361394\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b7_ns_512_3_0.8391 + Norm\n",
    "\n",
    "best_th=0.5\n",
    "best_cv=0.8524413710162358\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "convnext_large_384_in22ft1k_384_3_0.8200 + \n",
    "convnext_base_384_in22ft1k_512_3_0.8266 +\n",
    "tf_efficientnet_b5_ns_512_3_0.8384 + \n",
    "tf_efficientnet_b6_ns_512_3_0.8418 +\n",
    "tf_efficientnet_b7_ns_512_3_0.8391 + Norm\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8663359390659452\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "fold2:\n",
    "\n",
    "tf_efficientnet_b7_ns_512_2_0.8390 +\n",
    "tf_efficientnet_b5_ns_512_2_0.8403\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8569112046502304\n",
    "best_th_adjusted=0.9\n",
    "\n",
    "\n",
    "convnext_large_384_in22ft1k_384_2_0.8182 +\n",
    "tf_efficientnet_b7_ns_512_2_0.8390 +\n",
    "tf_efficientnet_b5_ns_512_2_0.8403 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8607095610342753\n",
    "best_th_adjusted=0.42\n",
    "\n",
    "convnext_large_384_in22ft1k_384_2_0.8182 +\n",
    "tf_efficientnet_b7_ns_512_2_0.8390 +\n",
    "tf_efficientnet_b6_ns_512_2_0.8439 +\n",
    "tf_efficientnet_b5_ns_512_2_0.8403 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.866738825415915\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "\n",
    "convnext_large_384_in22ft1k_384_2_0.8182 +\n",
    "convnext_base_384_in22ft1k_512_2_0.8163 +\n",
    "tf_efficientnet_b7_ns_512_2_0.8390 +\n",
    "tf_efficientnet_b6_ns_512_2_0.8439 +\n",
    "tf_efficientnet_b5_ns_512_2_0.8403 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8642513529765485\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "fold1:\n",
    "\n",
    "convnext_large_384_in22ft1k_384_1_0.8169 +\n",
    "tf_efficientnet_b5_ns_512_1_0.8351 +\n",
    "tf_efficientnet_b7_ns_512_1_0.8311 + NORM\n",
    "\n",
    "best_th=0.42\n",
    "best_cv=0.857147724994989\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "convnext_large_384_in22ft1k_384_1_0.8169 +\n",
    "tf_efficientnet_b5_ns_512_1_0.8351 +\n",
    "tf_efficientnet_b6_ns_512_1_0.8385 +\n",
    "tf_efficientnet_b7_ns_512_1_0.8311 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.861824012828222\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "\n",
    "tf_efficientnet_b5_ns_512_1_0.8351 +\n",
    "tf_efficientnet_b7_ns_512_1_0.8311\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8526618560833834\n",
    "best_th_adjusted=0.8\n",
    "\n",
    "\n",
    "convnext_large_384_in22ft1k_384_1_0.8169 +\n",
    "convnext_base_384_in22ft1k_512_1_0.8059 +\n",
    "tf_efficientnet_b5_ns_512_1_0.8351 +\n",
    "tf_efficientnet_b6_ns_512_1_0.8385 +\n",
    "tf_efficientnet_b7_ns_512_1_0.8311 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8610964121066347\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "\n",
    "fold0:\n",
    "\n",
    "convnext_large_384_in22ft1k_384_0_0.8216  +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 + NORM\n",
    "\n",
    "best_th=0.5\n",
    "best_cv=0.8509440769693325\n",
    "best_th_adjusted=0.4\n",
    "\n",
    "convnext_large_384_in22ft1k_384_0_0.8216  +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 \n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8561755862898376\n",
    "best_th_adjusted=0.8\n",
    "\n",
    "tf_efficientnet_b5_ns_512_0_0.8371 +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414\n",
    "\n",
    "best_th=0.9\n",
    "best_cv=0.8584606133493686\n",
    "best_th_adjusted=0.8\n",
    "\n",
    "convnext_large_384_in22ft1k_384_0_0.8216  +\n",
    "tf_efficientnet_b5_ns_512_0_0.8371 +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8632812186810984\n",
    "best_th_adjusted=0.42\n",
    "\n",
    "convnext_large_384_in22ft1k_384_0_0.8216  +\n",
    "tf_efficientnet_b5_ns_512_0_0.8371 +\n",
    "tf_efficientnet_b6_ns_512_0_0.8428 +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.867971537382241\n",
    "best_th_adjusted=0.42\n",
    "\n",
    "convnext_large_384_in22ft1k_384_0_0.8216  +\n",
    "convnext_base_384_in22ft1k_512_0_0.8149 + \n",
    "tf_efficientnet_b5_ns_512_0_0.8371 +\n",
    "tf_efficientnet_b6_ns_512_0_0.8428 +\n",
    "tf_efficientnet_b7_ns_512_0_0.8414 + NORM\n",
    "\n",
    "best_th=0.44\n",
    "best_cv=0.8668610944076969\n",
    "best_th_adjusted=0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3fb302-21f2-4c33-9df4-f2f51eb8be19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
