epoch: 80

LearningRate:
  base_lr: 0.0002
  schedulers:
  - name: PiecewiseDecay
    gamma: 0.1
    milestones:
    - 64
    - 86
  - name: LinearWarmup
    start_factor: 0.3333333333333333
    steps: 1000

OptimizerBuilder:
  regularizer: false
  optimizer:
    type: AdamW
    weight_decay: 0.0005